{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "question-twitter-basic-seq2seq (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7c3a162a-2e9a-459a-bc91-3a35008586b0",
        "_uuid": "355c54dccfd0e1dfd44e342f7ea8fa31353dce11",
        "colab_type": "text",
        "id": "563H-GuGGEvs"
      },
      "source": [
        "# Basic Seq2Seq for Twitter Customer Support\n",
        "\n",
        "In this assignment, we'll try to build a seq2seq chatbot that can mimic a customer support officer (how cool?!). The task we'll train it on is predicting company responses to consumers. The dataset we will use can be downloaded [here](https://chula-dl-class.s3-ap-southeast-1.amazonaws.com/customer-support-on-twitter.zip). \n",
        "\n",
        "Note that we'll only work on single-turn conversation, i.e. the response only depends on the most recent message from the customer. The multi-turn diaglog system is a much harder research problem and even the state-of-the-art models struggle to get it right. Therefore, it's not covered in this assignment.\n",
        "\n",
        "This notebook lets you prepare the data and construct a basic LSTM model in Keras. After a couple of hours of training on a GPU, your loss should get below 1.3 and the model starts to give sensible replies. To converge might take a bit longer depends on the complexity of your model.\n",
        "\n",
        "![seq2seq model architecture](https://i.imgur.com/JmuryKu.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-04toh5yGoQ",
        "colab_type": "code",
        "outputId": "db634397-3a25-4937-b91d-fbb4d07d281b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bfdzzEcWGEvt",
        "outputId": "74efeee9-1644-440c-ed45-ea46fc576db1",
        "colab": {}
      },
      "source": [
        "# installation. only run once\n",
        "!pip install langdetect\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "Requirement already satisfied: six in c:\\users\\rog\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from langdetect) (1.12.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py): started\n",
            "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp37-none-any.whl size=993467 sha256=0d8b87bfa7a807ca7ebe89d67d0dbb578f1b3a9218b4da098fadea5f881879b3\n",
            "  Stored in directory: C:\\Users\\ROG\\AppData\\Local\\pip\\Cache\\wheels\\ec\\0c\\a9\\1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.7\n",
            "Collecting sentencepiece\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/16/17838ebf03ee21daa3b4da0ca5c344bd060bc2963a7567a071cd7008e996/sentencepiece-0.1.83-cp37-cp37m-win_amd64.whl (1.2MB)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "39717eda-bc31-4143-a87f-bac4eea63678",
        "_uuid": "e30b174032d05dd0048b02ca268f3b31cf5b183e",
        "colab_type": "code",
        "id": "RGvgdQH0GEvx",
        "outputId": "64330fab-e17c-4708-91c3-6d2fddc14daa",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "\n",
        "print('Library versions:')\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "print(f'keras:{keras.__version__}')\n",
        "import pandas as pd\n",
        "print(f'pandas:{pd.__version__}')\n",
        "import numpy as np\n",
        "print(f'numpy:{np.__version__}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Library versions:\n",
            "keras:2.2.4-tf\n",
            "pandas:0.25.1\n",
            "numpy:1.16.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "24fb3871-ea0e-4b9e-b21c-ed76ccefc823",
        "_uuid": "ec0199c0dff22aeed45b9dbe1e7bf659e9449fe9",
        "colab_type": "text",
        "id": "e_WOoJk5GEvz"
      },
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6c_CYGkgOp0r",
        "colab": {}
      },
      "source": [
        "DATA_FOLDER = 'C:/Users/ROG/Desktop/data/customer-support-on-twitter/twcs/'\n",
        "#DATA_FOLDER = '/Users/admin/OneDrive - KNOREX/DL Class/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "3217b16f-bf7c-4fed-bbea-d74e09a537e4",
        "_uuid": "6509f5443163b0d11f1a003bb8a28b79a340f192",
        "colab_type": "code",
        "id": "lKrjabrgGEvz",
        "colab": {}
      },
      "source": [
        "# 4096 - large enough for demonstration, larger values make network training slower\n",
        "VOCAB_SIZE = 2**12\n",
        "# seq2seq generally relies on fixed length message vectors - longer messages provide more info\n",
        "# but result in slower training and larger networks\n",
        "MAX_MESSAGE_LEN = 50  \n",
        "# Embedding size for words - gives a trade off between expressivity of words and network size\n",
        "EMBEDDING_SIZE = 128\n",
        "# Embedding size for whole messages, same trade off as word embeddings\n",
        "CONTEXT_SIZE = 256\n",
        "# Larger batch sizes generally reach the average response faster, but small batch sizes are\n",
        "# required for the model to learn nuanced responses.  Also, GPU memory limits max batch size.\n",
        "bs = 32\n",
        "# Helps regularize network and prevent overfitting.\n",
        "DROPOUT = 0.2\n",
        "# High learning rate helps model reach average response faster, but can make it hard to \n",
        "# converge on nuanced responses\n",
        "LEARNING_RATE=0.005\n",
        "\n",
        "# Tokens needed for seq2seq\n",
        "PAD = 1  # after message has finished, this fills all remaining vector positions\n",
        "START = 2  # provided to the model at position 0 for every response predicted\n",
        "END = 3  # token that marks the end of the sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ea67c840-2d02-45df-be0b-ec5746e72028",
        "_uuid": "9f99990233daf26e624d6e0a735b8a3559eb54c9",
        "colab_type": "text",
        "id": "-scKAvNPGEv1"
      },
      "source": [
        "## Data Preparation\n",
        "### Data Loading and Reshaping\n",
        "\n",
        "Your first task is to extract the pairs of conversation from the input csv file. It has a column named `in_response_to_tweet_id`, which you can join with the `tweet_id` column to find pairs of conversations. Note that the original conversation is multi-turn. We want to extract the single-turn conversations where the first utterance is from the customer and the second one is from the company. E.g.: \n",
        "\n",
        "* customer: Hi Air Asia, I have a complaint! (1)\n",
        "* AirAsia: Sorry to hear that, can you provide more details (2)\n",
        "* customer: My flight got delayed for 12 hours! (3)\n",
        "* AirAsia: We apologize for that. We'll refund you in full. (4)\n",
        "* customer: Thanks! :) (5)\n",
        "\n",
        "So you shall extract the x, y pairs (1,2) and (3,4). Note that (5) is ignored since there's no reply from the company for it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c9f2bbcb-e843-42d9-81ec-70be9465c4a8",
        "_uuid": "0c3d1708c29764aceb980a93bd9ac61fdf852a22",
        "colab_type": "code",
        "id": "3J3gbuPyGEv2",
        "outputId": "1256ba52-be42-44ac-ecba-a283c8fa4994",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tweets = pd.read_csv(DATA_FOLDER + 'twcs.csv')\n",
        "tweets = tweets.fillna(-1).astype({'in_response_to_tweet_id':'int32'})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 8.36 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U0eYgxS_GEv3",
        "outputId": "5fbbc0e2-a8e2-4959-f10b-2c4dc73110b8",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "print('Total messages:', tweets.shape[0])\n",
        "tweets.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total messages: 2811774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>author_id</th>\n",
              "      <th>inbound</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>response_tweet_id</th>\n",
              "      <th>in_response_to_tweet_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 22:10:47 +0000 2017</td>\n",
              "      <td>@115712 I understand. I would like to assist y...</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:11:45 +0000 2017</td>\n",
              "      <td>@sprintcare and how do you propose we do that</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:08:27 +0000 2017</td>\n",
              "      <td>@sprintcare I have sent several private messag...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 21:54:49 +0000 2017</td>\n",
              "      <td>@115712 Please send us a Private Message so th...</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 21:49:35 +0000 2017</td>\n",
              "      <td>@sprintcare I did.</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tweet_id   author_id  inbound                      created_at  \\\n",
              "0         1  sprintcare    False  Tue Oct 31 22:10:47 +0000 2017   \n",
              "1         2      115712     True  Tue Oct 31 22:11:45 +0000 2017   \n",
              "2         3      115712     True  Tue Oct 31 22:08:27 +0000 2017   \n",
              "3         4  sprintcare    False  Tue Oct 31 21:54:49 +0000 2017   \n",
              "4         5      115712     True  Tue Oct 31 21:49:35 +0000 2017   \n",
              "\n",
              "                                                text response_tweet_id  \\\n",
              "0  @115712 I understand. I would like to assist y...                 2   \n",
              "1      @sprintcare and how do you propose we do that                -1   \n",
              "2  @sprintcare I have sent several private messag...                 1   \n",
              "3  @115712 Please send us a Private Message so th...                 3   \n",
              "4                                 @sprintcare I did.                 4   \n",
              "\n",
              "   in_response_to_tweet_id  \n",
              "0                        3  \n",
              "1                        1  \n",
              "2                        4  \n",
              "3                        5  \n",
              "4                        6  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DdCt5qAcSz0R",
        "colab": {}
      },
      "source": [
        "# TODO: get the outbound messages which response to some user query. Hint, a simple pandas filter will do.\n",
        "outbounds=tweets.query('inbound==`False`')\n",
        "inbounds=tweets.query('inbound==`True`')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AEwsMvWKGEv5",
        "outputId": "a97bbe3c-ab2f-480d-93d2-7bc7e78b5cf1",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "print('Total replies:', outbounds.shape[0])\n",
        "outbounds.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total replies: 1273931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>author_id</th>\n",
              "      <th>inbound</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>response_tweet_id</th>\n",
              "      <th>in_response_to_tweet_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 22:10:47 +0000 2017</td>\n",
              "      <td>@115712 I understand. I would like to assist y...</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 21:54:49 +0000 2017</td>\n",
              "      <td>@115712 Please send us a Private Message so th...</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 21:46:24 +0000 2017</td>\n",
              "      <td>@115712 Can you please send us a private messa...</td>\n",
              "      <td>5,7</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 22:10:35 +0000 2017</td>\n",
              "      <td>@115713 This is saddening to hear. Please shoo...</td>\n",
              "      <td>-1</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>15</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 20:03:31 +0000 2017</td>\n",
              "      <td>@115713 We understand your concerns and we'd l...</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tweet_id   author_id  inbound                      created_at  \\\n",
              "0         1  sprintcare    False  Tue Oct 31 22:10:47 +0000 2017   \n",
              "3         4  sprintcare    False  Tue Oct 31 21:54:49 +0000 2017   \n",
              "5         6  sprintcare    False  Tue Oct 31 21:46:24 +0000 2017   \n",
              "7        11  sprintcare    False  Tue Oct 31 22:10:35 +0000 2017   \n",
              "9        15  sprintcare    False  Tue Oct 31 20:03:31 +0000 2017   \n",
              "\n",
              "                                                text response_tweet_id  \\\n",
              "0  @115712 I understand. I would like to assist y...                 2   \n",
              "3  @115712 Please send us a Private Message so th...                 3   \n",
              "5  @115712 Can you please send us a private messa...               5,7   \n",
              "7  @115713 This is saddening to hear. Please shoo...                -1   \n",
              "9  @115713 We understand your concerns and we'd l...                12   \n",
              "\n",
              "   in_response_to_tweet_id  \n",
              "0                        3  \n",
              "3                        5  \n",
              "5                        8  \n",
              "7                       12  \n",
              "9                       16  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-6-rN5qZoyD",
        "colab_type": "code",
        "outputId": "290fa312-54e8-438a-e911-131e74d40e28",
        "colab": {}
      },
      "source": [
        "print('Total replies:', outbounds.shape[0])\n",
        "inbounds.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total replies: 1273931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>author_id</th>\n",
              "      <th>inbound</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>response_tweet_id</th>\n",
              "      <th>in_response_to_tweet_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:11:45 +0000 2017</td>\n",
              "      <td>@sprintcare and how do you propose we do that</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:08:27 +0000 2017</td>\n",
              "      <td>@sprintcare I have sent several private messag...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 21:49:35 +0000 2017</td>\n",
              "      <td>@sprintcare I did.</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n",
              "      <td>@sprintcare is the worst customer service</td>\n",
              "      <td>9,6,10</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>115713</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:04:47 +0000 2017</td>\n",
              "      <td>@sprintcare You gonna magically change your co...</td>\n",
              "      <td>11,13,14</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tweet_id author_id  inbound                      created_at  \\\n",
              "1         2    115712     True  Tue Oct 31 22:11:45 +0000 2017   \n",
              "2         3    115712     True  Tue Oct 31 22:08:27 +0000 2017   \n",
              "4         5    115712     True  Tue Oct 31 21:49:35 +0000 2017   \n",
              "6         8    115712     True  Tue Oct 31 21:45:10 +0000 2017   \n",
              "8        12    115713     True  Tue Oct 31 22:04:47 +0000 2017   \n",
              "\n",
              "                                                text response_tweet_id  \\\n",
              "1      @sprintcare and how do you propose we do that                -1   \n",
              "2  @sprintcare I have sent several private messag...                 1   \n",
              "4                                 @sprintcare I did.                 4   \n",
              "6          @sprintcare is the worst customer service            9,6,10   \n",
              "8  @sprintcare You gonna magically change your co...          11,13,14   \n",
              "\n",
              "   in_response_to_tweet_id  \n",
              "1                        1  \n",
              "2                        4  \n",
              "4                        6  \n",
              "6                       -1  \n",
              "8                       15  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BDUQVxotZoyH",
        "colab_type": "code",
        "outputId": "435e9e24-1b36-4d20-89af-0805f56ed93e",
        "colab": {}
      },
      "source": [
        "inbounds_and_outbounds = tweets.merge(outbounds, left_on='tweet_id', right_on='in_response_to_tweet_id')\n",
        "inbounds_and_outbounds.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id_x</th>\n",
              "      <th>author_id_x</th>\n",
              "      <th>inbound_x</th>\n",
              "      <th>created_at_x</th>\n",
              "      <th>text_x</th>\n",
              "      <th>response_tweet_id_x</th>\n",
              "      <th>in_response_to_tweet_id_x</th>\n",
              "      <th>tweet_id_y</th>\n",
              "      <th>author_id_y</th>\n",
              "      <th>inbound_y</th>\n",
              "      <th>created_at_y</th>\n",
              "      <th>text_y</th>\n",
              "      <th>response_tweet_id_y</th>\n",
              "      <th>in_response_to_tweet_id_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:08:27 +0000 2017</td>\n",
              "      <td>@sprintcare I have sent several private messag...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 22:10:47 +0000 2017</td>\n",
              "      <td>@115712 I understand. I would like to assist y...</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 21:49:35 +0000 2017</td>\n",
              "      <td>@sprintcare I did.</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 21:54:49 +0000 2017</td>\n",
              "      <td>@115712 Please send us a Private Message so th...</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n",
              "      <td>@sprintcare is the worst customer service</td>\n",
              "      <td>9,6,10</td>\n",
              "      <td>-1</td>\n",
              "      <td>6</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 21:46:24 +0000 2017</td>\n",
              "      <td>@115712 Can you please send us a private messa...</td>\n",
              "      <td>5,7</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n",
              "      <td>@sprintcare is the worst customer service</td>\n",
              "      <td>9,6,10</td>\n",
              "      <td>-1</td>\n",
              "      <td>9</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 21:46:14 +0000 2017</td>\n",
              "      <td>@115712 I would love the chance to review the ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n",
              "      <td>@sprintcare is the worst customer service</td>\n",
              "      <td>9,6,10</td>\n",
              "      <td>-1</td>\n",
              "      <td>10</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 21:45:59 +0000 2017</td>\n",
              "      <td>@115712 Hello! We never like our customers to ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>115713</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:04:47 +0000 2017</td>\n",
              "      <td>@sprintcare You gonna magically change your co...</td>\n",
              "      <td>11,13,14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 22:10:35 +0000 2017</td>\n",
              "      <td>@115713 This is saddening to hear. Please shoo...</td>\n",
              "      <td>-1</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>115713</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:04:47 +0000 2017</td>\n",
              "      <td>@sprintcare You gonna magically change your co...</td>\n",
              "      <td>11,13,14</td>\n",
              "      <td>15</td>\n",
              "      <td>13</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Wed Nov 01 20:48:14 +0000 2017</td>\n",
              "      <td>@115713 I would really like to work with you t...</td>\n",
              "      <td>-1</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>115713</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:04:47 +0000 2017</td>\n",
              "      <td>@sprintcare You gonna magically change your co...</td>\n",
              "      <td>11,13,14</td>\n",
              "      <td>15</td>\n",
              "      <td>14</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Wed Nov 01 20:47:40 +0000 2017</td>\n",
              "      <td>@115713 Hi, my name is Shantel, I'm a resoluti...</td>\n",
              "      <td>-1</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>115713</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 20:00:43 +0000 2017</td>\n",
              "      <td>@sprintcare Since I signed up with you....Sinc...</td>\n",
              "      <td>15</td>\n",
              "      <td>17</td>\n",
              "      <td>15</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 20:03:31 +0000 2017</td>\n",
              "      <td>@115713 We understand your concerns and we'd l...</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>115713</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 19:56:01 +0000 2017</td>\n",
              "      <td>@115714 y’all lie about your “great” connectio...</td>\n",
              "      <td>17</td>\n",
              "      <td>-1</td>\n",
              "      <td>17</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 19:59:13 +0000 2017</td>\n",
              "      <td>@115713 H there! We'd definitely like to work ...</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tweet_id_x author_id_x  inbound_x                    created_at_x  \\\n",
              "0           3      115712       True  Tue Oct 31 22:08:27 +0000 2017   \n",
              "1           5      115712       True  Tue Oct 31 21:49:35 +0000 2017   \n",
              "2           8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n",
              "3           8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n",
              "4           8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n",
              "5          12      115713       True  Tue Oct 31 22:04:47 +0000 2017   \n",
              "6          12      115713       True  Tue Oct 31 22:04:47 +0000 2017   \n",
              "7          12      115713       True  Tue Oct 31 22:04:47 +0000 2017   \n",
              "8          16      115713       True  Tue Oct 31 20:00:43 +0000 2017   \n",
              "9          18      115713       True  Tue Oct 31 19:56:01 +0000 2017   \n",
              "\n",
              "                                              text_x response_tweet_id_x  \\\n",
              "0  @sprintcare I have sent several private messag...                   1   \n",
              "1                                 @sprintcare I did.                   4   \n",
              "2          @sprintcare is the worst customer service              9,6,10   \n",
              "3          @sprintcare is the worst customer service              9,6,10   \n",
              "4          @sprintcare is the worst customer service              9,6,10   \n",
              "5  @sprintcare You gonna magically change your co...            11,13,14   \n",
              "6  @sprintcare You gonna magically change your co...            11,13,14   \n",
              "7  @sprintcare You gonna magically change your co...            11,13,14   \n",
              "8  @sprintcare Since I signed up with you....Sinc...                  15   \n",
              "9  @115714 y’all lie about your “great” connectio...                  17   \n",
              "\n",
              "   in_response_to_tweet_id_x  tweet_id_y author_id_y  inbound_y  \\\n",
              "0                          4           1  sprintcare      False   \n",
              "1                          6           4  sprintcare      False   \n",
              "2                         -1           6  sprintcare      False   \n",
              "3                         -1           9  sprintcare      False   \n",
              "4                         -1          10  sprintcare      False   \n",
              "5                         15          11  sprintcare      False   \n",
              "6                         15          13  sprintcare      False   \n",
              "7                         15          14  sprintcare      False   \n",
              "8                         17          15  sprintcare      False   \n",
              "9                         -1          17  sprintcare      False   \n",
              "\n",
              "                     created_at_y  \\\n",
              "0  Tue Oct 31 22:10:47 +0000 2017   \n",
              "1  Tue Oct 31 21:54:49 +0000 2017   \n",
              "2  Tue Oct 31 21:46:24 +0000 2017   \n",
              "3  Tue Oct 31 21:46:14 +0000 2017   \n",
              "4  Tue Oct 31 21:45:59 +0000 2017   \n",
              "5  Tue Oct 31 22:10:35 +0000 2017   \n",
              "6  Wed Nov 01 20:48:14 +0000 2017   \n",
              "7  Wed Nov 01 20:47:40 +0000 2017   \n",
              "8  Tue Oct 31 20:03:31 +0000 2017   \n",
              "9  Tue Oct 31 19:59:13 +0000 2017   \n",
              "\n",
              "                                              text_y response_tweet_id_y  \\\n",
              "0  @115712 I understand. I would like to assist y...                   2   \n",
              "1  @115712 Please send us a Private Message so th...                   3   \n",
              "2  @115712 Can you please send us a private messa...                 5,7   \n",
              "3  @115712 I would love the chance to review the ...                  -1   \n",
              "4  @115712 Hello! We never like our customers to ...                  -1   \n",
              "5  @115713 This is saddening to hear. Please shoo...                  -1   \n",
              "6  @115713 I would really like to work with you t...                  -1   \n",
              "7  @115713 Hi, my name is Shantel, I'm a resoluti...                  -1   \n",
              "8  @115713 We understand your concerns and we'd l...                  12   \n",
              "9  @115713 H there! We'd definitely like to work ...                  16   \n",
              "\n",
              "   in_response_to_tweet_id_y  \n",
              "0                          3  \n",
              "1                          5  \n",
              "2                          8  \n",
              "3                          8  \n",
              "4                          8  \n",
              "5                         12  \n",
              "6                         12  \n",
              "7                         12  \n",
              "8                         16  \n",
              "9                         18  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ht-wfF05S75B",
        "outputId": "3dd9b9ff-a3ae-4ee9-9c3d-74df96ba6262",
        "colab": {}
      },
      "source": [
        "# TODO: join the conversation together with x as the user query and y as the company's response. Also randomize the data\n",
        "# Hint: 1. you can use pandas.merge with the correct id column. 2. you can use pandas.sample with frac=1 to randomize the data\n",
        "inbounds_and_outbounds = inbounds_and_outbounds.sample(frac =1)\n",
        "inbounds_and_outbounds.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id_x</th>\n",
              "      <th>author_id_x</th>\n",
              "      <th>inbound_x</th>\n",
              "      <th>created_at_x</th>\n",
              "      <th>text_x</th>\n",
              "      <th>response_tweet_id_x</th>\n",
              "      <th>in_response_to_tweet_id_x</th>\n",
              "      <th>tweet_id_y</th>\n",
              "      <th>author_id_y</th>\n",
              "      <th>inbound_y</th>\n",
              "      <th>created_at_y</th>\n",
              "      <th>text_y</th>\n",
              "      <th>response_tweet_id_y</th>\n",
              "      <th>in_response_to_tweet_id_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1128951</td>\n",
              "      <td>2670651</td>\n",
              "      <td>752079</td>\n",
              "      <td>True</td>\n",
              "      <td>Sun Nov 19 10:09:10 +0000 2017</td>\n",
              "      <td>Stuck on a @VirginAtlantic plane at Gatwick be...</td>\n",
              "      <td>2670649</td>\n",
              "      <td>-1</td>\n",
              "      <td>2670649</td>\n",
              "      <td>VirginAtlantic</td>\n",
              "      <td>False</td>\n",
              "      <td>Sun Nov 19 10:17:11 +0000 2017</td>\n",
              "      <td>@752079 Sorry to hear that Brian. Which flight...</td>\n",
              "      <td>2670650</td>\n",
              "      <td>2670651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1224025</td>\n",
              "      <td>2891678</td>\n",
              "      <td>801590</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Nov 28 04:00:48 +0000 2017</td>\n",
              "      <td>@116035: There is a payment I made to Green Mo...</td>\n",
              "      <td>2891677,2891679</td>\n",
              "      <td>-1</td>\n",
              "      <td>2891677</td>\n",
              "      <td>BofA_Help</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Nov 28 14:36:15 +0000 2017</td>\n",
              "      <td>@801590 Pls click below to DM your name/ZIP/ph...</td>\n",
              "      <td>-1</td>\n",
              "      <td>2891678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>792064</td>\n",
              "      <td>1888285</td>\n",
              "      <td>563141</td>\n",
              "      <td>True</td>\n",
              "      <td>Mon Oct 23 15:07:24 +0000 2017</td>\n",
              "      <td>@AppleSupport fix this restarting problem plea...</td>\n",
              "      <td>1888283</td>\n",
              "      <td>-1</td>\n",
              "      <td>1888283</td>\n",
              "      <td>AppleSupport</td>\n",
              "      <td>False</td>\n",
              "      <td>Mon Oct 23 15:36:00 +0000 2017</td>\n",
              "      <td>@563141 Send us a DM and tell us what issues y...</td>\n",
              "      <td>1888284</td>\n",
              "      <td>1888285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>657724</td>\n",
              "      <td>1573478</td>\n",
              "      <td>131957</td>\n",
              "      <td>True</td>\n",
              "      <td>Mon Oct 16 22:22:48 +0000 2017</td>\n",
              "      <td>@GWRHelp Will you consider pre booked seats on...</td>\n",
              "      <td>1573476,1573494</td>\n",
              "      <td>-1</td>\n",
              "      <td>1573476</td>\n",
              "      <td>GWRHelp</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 17 06:18:12 +0000 2017</td>\n",
              "      <td>@131957 Hi there. Can you please explain what ...</td>\n",
              "      <td>1573477</td>\n",
              "      <td>1573478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1094823</td>\n",
              "      <td>2592594</td>\n",
              "      <td>734240</td>\n",
              "      <td>True</td>\n",
              "      <td>Fri Nov 17 01:52:34 +0000 2017</td>\n",
              "      <td>@idea_cares why is it compulsory to recharge w...</td>\n",
              "      <td>2592593,2592595</td>\n",
              "      <td>-1</td>\n",
              "      <td>2592593</td>\n",
              "      <td>idea_cares</td>\n",
              "      <td>False</td>\n",
              "      <td>Fri Nov 17 01:58:01 +0000 2017</td>\n",
              "      <td>@734240 We would request you to share your ide...</td>\n",
              "      <td>-1</td>\n",
              "      <td>2592594</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         tweet_id_x author_id_x  inbound_x                    created_at_x  \\\n",
              "1128951     2670651      752079       True  Sun Nov 19 10:09:10 +0000 2017   \n",
              "1224025     2891678      801590       True  Tue Nov 28 04:00:48 +0000 2017   \n",
              "792064      1888285      563141       True  Mon Oct 23 15:07:24 +0000 2017   \n",
              "657724      1573478      131957       True  Mon Oct 16 22:22:48 +0000 2017   \n",
              "1094823     2592594      734240       True  Fri Nov 17 01:52:34 +0000 2017   \n",
              "\n",
              "                                                    text_x  \\\n",
              "1128951  Stuck on a @VirginAtlantic plane at Gatwick be...   \n",
              "1224025  @116035: There is a payment I made to Green Mo...   \n",
              "792064   @AppleSupport fix this restarting problem plea...   \n",
              "657724   @GWRHelp Will you consider pre booked seats on...   \n",
              "1094823  @idea_cares why is it compulsory to recharge w...   \n",
              "\n",
              "        response_tweet_id_x  in_response_to_tweet_id_x  tweet_id_y  \\\n",
              "1128951             2670649                         -1     2670649   \n",
              "1224025     2891677,2891679                         -1     2891677   \n",
              "792064              1888283                         -1     1888283   \n",
              "657724      1573476,1573494                         -1     1573476   \n",
              "1094823     2592593,2592595                         -1     2592593   \n",
              "\n",
              "            author_id_y  inbound_y                    created_at_y  \\\n",
              "1128951  VirginAtlantic      False  Sun Nov 19 10:17:11 +0000 2017   \n",
              "1224025       BofA_Help      False  Tue Nov 28 14:36:15 +0000 2017   \n",
              "792064     AppleSupport      False  Mon Oct 23 15:36:00 +0000 2017   \n",
              "657724          GWRHelp      False  Tue Oct 17 06:18:12 +0000 2017   \n",
              "1094823      idea_cares      False  Fri Nov 17 01:58:01 +0000 2017   \n",
              "\n",
              "                                                    text_y  \\\n",
              "1128951  @752079 Sorry to hear that Brian. Which flight...   \n",
              "1224025  @801590 Pls click below to DM your name/ZIP/ph...   \n",
              "792064   @563141 Send us a DM and tell us what issues y...   \n",
              "657724   @131957 Hi there. Can you please explain what ...   \n",
              "1094823  @734240 We would request you to share your ide...   \n",
              "\n",
              "        response_tweet_id_y  in_response_to_tweet_id_y  \n",
              "1128951             2670650                    2670651  \n",
              "1224025                  -1                    2891678  \n",
              "792064              1888284                    1888285  \n",
              "657724              1573477                    1573478  \n",
              "1094823                  -1                    2592594  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jKkjHYRaGEv8",
        "outputId": "247be318-57dd-4606-fd16-bf5528dcfdfc",
        "colab": {}
      },
      "source": [
        "# tqdm().pandas()  # Enable tracking of progress in dataframe `apply` calls\n",
        "print('Conversation turns:', inbounds_and_outbounds.shape[0])\n",
        "inbounds_and_outbounds.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conversation turns: 1265281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id_x</th>\n",
              "      <th>author_id_x</th>\n",
              "      <th>inbound_x</th>\n",
              "      <th>created_at_x</th>\n",
              "      <th>text_x</th>\n",
              "      <th>response_tweet_id_x</th>\n",
              "      <th>in_response_to_tweet_id_x</th>\n",
              "      <th>tweet_id_y</th>\n",
              "      <th>author_id_y</th>\n",
              "      <th>inbound_y</th>\n",
              "      <th>created_at_y</th>\n",
              "      <th>text_y</th>\n",
              "      <th>response_tweet_id_y</th>\n",
              "      <th>in_response_to_tweet_id_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1128951</td>\n",
              "      <td>2670651</td>\n",
              "      <td>752079</td>\n",
              "      <td>True</td>\n",
              "      <td>Sun Nov 19 10:09:10 +0000 2017</td>\n",
              "      <td>Stuck on a @VirginAtlantic plane at Gatwick be...</td>\n",
              "      <td>2670649</td>\n",
              "      <td>-1</td>\n",
              "      <td>2670649</td>\n",
              "      <td>VirginAtlantic</td>\n",
              "      <td>False</td>\n",
              "      <td>Sun Nov 19 10:17:11 +0000 2017</td>\n",
              "      <td>@752079 Sorry to hear that Brian. Which flight...</td>\n",
              "      <td>2670650</td>\n",
              "      <td>2670651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1224025</td>\n",
              "      <td>2891678</td>\n",
              "      <td>801590</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Nov 28 04:00:48 +0000 2017</td>\n",
              "      <td>@116035: There is a payment I made to Green Mo...</td>\n",
              "      <td>2891677,2891679</td>\n",
              "      <td>-1</td>\n",
              "      <td>2891677</td>\n",
              "      <td>BofA_Help</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Nov 28 14:36:15 +0000 2017</td>\n",
              "      <td>@801590 Pls click below to DM your name/ZIP/ph...</td>\n",
              "      <td>-1</td>\n",
              "      <td>2891678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>792064</td>\n",
              "      <td>1888285</td>\n",
              "      <td>563141</td>\n",
              "      <td>True</td>\n",
              "      <td>Mon Oct 23 15:07:24 +0000 2017</td>\n",
              "      <td>@AppleSupport fix this restarting problem plea...</td>\n",
              "      <td>1888283</td>\n",
              "      <td>-1</td>\n",
              "      <td>1888283</td>\n",
              "      <td>AppleSupport</td>\n",
              "      <td>False</td>\n",
              "      <td>Mon Oct 23 15:36:00 +0000 2017</td>\n",
              "      <td>@563141 Send us a DM and tell us what issues y...</td>\n",
              "      <td>1888284</td>\n",
              "      <td>1888285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>657724</td>\n",
              "      <td>1573478</td>\n",
              "      <td>131957</td>\n",
              "      <td>True</td>\n",
              "      <td>Mon Oct 16 22:22:48 +0000 2017</td>\n",
              "      <td>@GWRHelp Will you consider pre booked seats on...</td>\n",
              "      <td>1573476,1573494</td>\n",
              "      <td>-1</td>\n",
              "      <td>1573476</td>\n",
              "      <td>GWRHelp</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 17 06:18:12 +0000 2017</td>\n",
              "      <td>@131957 Hi there. Can you please explain what ...</td>\n",
              "      <td>1573477</td>\n",
              "      <td>1573478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1094823</td>\n",
              "      <td>2592594</td>\n",
              "      <td>734240</td>\n",
              "      <td>True</td>\n",
              "      <td>Fri Nov 17 01:52:34 +0000 2017</td>\n",
              "      <td>@idea_cares why is it compulsory to recharge w...</td>\n",
              "      <td>2592593,2592595</td>\n",
              "      <td>-1</td>\n",
              "      <td>2592593</td>\n",
              "      <td>idea_cares</td>\n",
              "      <td>False</td>\n",
              "      <td>Fri Nov 17 01:58:01 +0000 2017</td>\n",
              "      <td>@734240 We would request you to share your ide...</td>\n",
              "      <td>-1</td>\n",
              "      <td>2592594</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         tweet_id_x author_id_x  inbound_x                    created_at_x  \\\n",
              "1128951     2670651      752079       True  Sun Nov 19 10:09:10 +0000 2017   \n",
              "1224025     2891678      801590       True  Tue Nov 28 04:00:48 +0000 2017   \n",
              "792064      1888285      563141       True  Mon Oct 23 15:07:24 +0000 2017   \n",
              "657724      1573478      131957       True  Mon Oct 16 22:22:48 +0000 2017   \n",
              "1094823     2592594      734240       True  Fri Nov 17 01:52:34 +0000 2017   \n",
              "\n",
              "                                                    text_x  \\\n",
              "1128951  Stuck on a @VirginAtlantic plane at Gatwick be...   \n",
              "1224025  @116035: There is a payment I made to Green Mo...   \n",
              "792064   @AppleSupport fix this restarting problem plea...   \n",
              "657724   @GWRHelp Will you consider pre booked seats on...   \n",
              "1094823  @idea_cares why is it compulsory to recharge w...   \n",
              "\n",
              "        response_tweet_id_x  in_response_to_tweet_id_x  tweet_id_y  \\\n",
              "1128951             2670649                         -1     2670649   \n",
              "1224025     2891677,2891679                         -1     2891677   \n",
              "792064              1888283                         -1     1888283   \n",
              "657724      1573476,1573494                         -1     1573476   \n",
              "1094823     2592593,2592595                         -1     2592593   \n",
              "\n",
              "            author_id_y  inbound_y                    created_at_y  \\\n",
              "1128951  VirginAtlantic      False  Sun Nov 19 10:17:11 +0000 2017   \n",
              "1224025       BofA_Help      False  Tue Nov 28 14:36:15 +0000 2017   \n",
              "792064     AppleSupport      False  Mon Oct 23 15:36:00 +0000 2017   \n",
              "657724          GWRHelp      False  Tue Oct 17 06:18:12 +0000 2017   \n",
              "1094823      idea_cares      False  Fri Nov 17 01:58:01 +0000 2017   \n",
              "\n",
              "                                                    text_y  \\\n",
              "1128951  @752079 Sorry to hear that Brian. Which flight...   \n",
              "1224025  @801590 Pls click below to DM your name/ZIP/ph...   \n",
              "792064   @563141 Send us a DM and tell us what issues y...   \n",
              "657724   @131957 Hi there. Can you please explain what ...   \n",
              "1094823  @734240 We would request you to share your ide...   \n",
              "\n",
              "        response_tweet_id_y  in_response_to_tweet_id_y  \n",
              "1128951             2670650                    2670651  \n",
              "1224025                  -1                    2891678  \n",
              "792064              1888284                    1888285  \n",
              "657724              1573477                    1573478  \n",
              "1094823                  -1                    2592594  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1510aa86-71ce-41b6-bda9-6bec1206836b",
        "_uuid": "67186b1ace1e3fff0fed9ec9fe6fd0c59f009a5b",
        "colab_type": "code",
        "id": "22ZfCy-wGEv9",
        "colab": {}
      },
      "source": [
        "inbounds_and_outbounds.to_csv(DATA_FOLDER + 'tweets_conversations.csv', index=False, columns=['text_x', 'text_y', 'author_id_y'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a0afc1aa-a34f-4ea5-b34e-2fb21b5da62e",
        "_uuid": "2478590bbc0924cc8e8dd127ac4fea8019f8b3f3",
        "colab_type": "text",
        "id": "4SyGZHhxGEv_"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oXyLRON8GEwA",
        "colab": {}
      },
      "source": [
        "inbounds_and_outbounds = pd.read_csv(DATA_FOLDER + 'tweets_conversations.csv', lineterminator='\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "cab88942-373b-42fa-ac15-f341b19ced6f",
        "_uuid": "eccc4fdc8c0770ccb650c2fea30d7c4c5644ee4b",
        "colab_type": "code",
        "id": "er6OfX7bGEwC",
        "outputId": "c3ec2bf7-52ba-4d76-9f26-99f28ea331b9",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "inbounds_and_outbounds.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_x</th>\n",
              "      <th>text_y</th>\n",
              "      <th>author_id_y\\r</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>Stuck on a @VirginAtlantic plane at Gatwick be...</td>\n",
              "      <td>@752079 Sorry to hear that Brian. Which flight...</td>\n",
              "      <td>VirginAtlantic\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>@116035: There is a payment I made to Green Mo...</td>\n",
              "      <td>@801590 Pls click below to DM your name/ZIP/ph...</td>\n",
              "      <td>BofA_Help\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>@AppleSupport fix this restarting problem plea...</td>\n",
              "      <td>@563141 Send us a DM and tell us what issues y...</td>\n",
              "      <td>AppleSupport\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>@GWRHelp Will you consider pre booked seats on...</td>\n",
              "      <td>@131957 Hi there. Can you please explain what ...</td>\n",
              "      <td>GWRHelp\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>@idea_cares why is it compulsory to recharge w...</td>\n",
              "      <td>@734240 We would request you to share your ide...</td>\n",
              "      <td>idea_cares\\r</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              text_x  \\\n",
              "0  Stuck on a @VirginAtlantic plane at Gatwick be...   \n",
              "1  @116035: There is a payment I made to Green Mo...   \n",
              "2  @AppleSupport fix this restarting problem plea...   \n",
              "3  @GWRHelp Will you consider pre booked seats on...   \n",
              "4  @idea_cares why is it compulsory to recharge w...   \n",
              "\n",
              "                                              text_y     author_id_y\\r  \n",
              "0  @752079 Sorry to hear that Brian. Which flight...  VirginAtlantic\\r  \n",
              "1  @801590 Pls click below to DM your name/ZIP/ph...       BofA_Help\\r  \n",
              "2  @563141 Send us a DM and tell us what issues y...    AppleSupport\\r  \n",
              "3  @131957 Hi there. Can you please explain what ...         GWRHelp\\r  \n",
              "4  @734240 We would request you to share your ide...      idea_cares\\r  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AbdN4f0hGEwE"
      },
      "source": [
        "#### 1. Data cleaning\n",
        "\n",
        "Cleaning up the data reduces the noise and the vocabulary size, which is critical to make sure you train a highly accurate model. I recommend doing the following data cleaning steps (feel free to add other steps).\n",
        "\n",
        "1. Filter out non-English tweets such as:\n",
        "\n",
        "\n",
        "| Inbound | Outbound |\n",
        "| -----| ------|\n",
        "|@115798 อยากทราบว่า ผู้สูงอายุ ที่มีโรคประจำตัว สามารถขึ้นเครื่องได้หรือป่าว | \"@420634 Hi, kindly tweet us in English or you may contact Thailand Call Centre +66 2 515 9999 for further assistance. Thanks - Ed\"|\n",
        "\n",
        "This can be done using a language detection library, such as https://pypi.org/project/langdetect/. For simplicity, you can concatenate the inbound and outbound message and pass to the model to predict. Removing other languages helps us to reduce the unnecessary vocabulary. Since they don't have enough data, we can't build a good chatbot for these languages anyway.\n",
        "\n",
        "2. Replace all screen names like `@12343` with `@__sn__`. My conjecture is that keep the screen name of the company (e.g. `@SW_Help`) may help since each company may respond differently.\n",
        "\n",
        "3. Remove all the non-ascii characters (e.g. emojis 🤥 💯,🔥).\n",
        "\n",
        "4. Lowercase all tweets. The captalization in informal text is noisy anyway and they don't prevent from understanding\n",
        "\n",
        "5. Replace all the URLs with a <URL> token.\n",
        "    \n",
        "6. Remove all hashtags, since they are often out of context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dm7oOLbYGEwF",
        "scrolled": true,
        "outputId": "dab76127-0765-46d7-834b-7ace1b5dc8d4",
        "colab": {
          "referenced_widgets": [
            "5aee407fc63e476ea0f6651f8e4ac979",
            "57f763d7e4e84129ae6e9dd0979b0df9",
            "79cdb1cf60164b69a15fa4eb327a9fc4"
          ]
        }
      },
      "source": [
        "%%time\n",
        "tqdm().pandas()\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "# TODO: filter out none English content\n",
        "# Note: it takes around 1hr to filter all the data in the corpus. \n",
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "# i =0\n",
        "# for index, row in tqdm(inbounds_and_outbounds.iterrows()):\n",
        "#     try :\n",
        "#         check = detect(row['text_x'])\n",
        "#     except :\n",
        "#         inbounds_and_outbounds.drop(index)\n",
        "# #         print(row['text_x'])\n",
        "#         continue\n",
        "# #     if i == 10 :\n",
        "# #         break\n",
        "#     if check!='en':\n",
        "# #         print(row['text_x'])\n",
        "#         inbounds_and_outbounds.drop(index)\n",
        "# #         print(\"dropped\")\n",
        "# #     i+=1    \n",
        "\n",
        "def detect_lang(query):\n",
        "    try:\n",
        "        lang = detect(query)\n",
        "    except LangDetectException:\n",
        "        lang = None\n",
        "    return lang\n",
        "\n",
        "df_en = inbounds_and_outbounds.copy()\n",
        "df_en = df_en[df_en.text_x.progress_apply(lambda x : detect_lang(x))=='en']\n",
        "df_en = df_en[df_en.text_y.progress_apply(lambda x : detect_lang(x))=='en']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5aee407fc63e476ea0f6651f8e4ac979",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57f763d7e4e84129ae6e9dd0979b0df9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1265281), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79cdb1cf60164b69a15fa4eb327a9fc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1169765), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Wall time: 2h 35min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EjzKWCo3GEwl",
        "colab": {}
      },
      "source": [
        "df_en.to_csv(DATA_FOLDER + 'tweets_conversations_en.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PtRloLgvGEwn",
        "colab": {}
      },
      "source": [
        "df_en = pd.read_csv(DATA_FOLDER + 'tweets_conversations_en.csv', lineterminator='\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Sqv-KDOBZoyW",
        "colab_type": "code",
        "outputId": "b4233773-a9b9-448c-aa2f-ac5a18455660",
        "colab": {}
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py): started\n",
            "  Building wheel for emoji (setup.py): finished with status 'done'\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp37-none-any.whl size=42181 sha256=5ca69b9c5fccb3b18b057d24326d9b11a45328e72b73a2cc49eae9ef5f57f085\n",
            "  Stored in directory: C:\\Users\\ROG\\AppData\\Local\\pip\\Cache\\wheels\\2a\\a9\\0a\\4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOwez1drZoya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMOTICONS = {\n",
        "    u\":‑\\)\":\"Happy face or smiley\",\n",
        "    u\":\\)\":\"Happy face or smiley\",\n",
        "    u\":-\\]\":\"Happy face or smiley\",\n",
        "    u\":\\]\":\"Happy face or smiley\",\n",
        "    u\":-3\":\"Happy face smiley\",\n",
        "    u\":3\":\"Happy face smiley\",\n",
        "    u\":->\":\"Happy face smiley\",\n",
        "    u\":>\":\"Happy face smiley\",\n",
        "    u\"8-\\)\":\"Happy face smiley\",\n",
        "    u\":o\\)\":\"Happy face smiley\",\n",
        "    u\":-\\}\":\"Happy face smiley\",\n",
        "    u\":\\}\":\"Happy face smiley\",\n",
        "    u\":-\\)\":\"Happy face smiley\",\n",
        "    u\":c\\)\":\"Happy face smiley\",\n",
        "    u\":\\^\\)\":\"Happy face smiley\",\n",
        "    u\"=\\]\":\"Happy face smiley\",\n",
        "    u\"=\\)\":\"Happy face smiley\",\n",
        "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":-\\)\\)\":\"Very happy\",\n",
        "    u\":‑\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑c\":\"Frown, sad, andry or pouting\",\n",
        "    u\":c\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
        "    u\":@\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":'‑\\(\":\"Crying\",\n",
        "    u\":'\\(\":\"Crying\",\n",
        "    u\":'‑\\)\":\"Tears of happiness\",\n",
        "    u\":'\\)\":\"Tears of happiness\",\n",
        "    u\"D‑':\":\"Horror\",\n",
        "    u\"D:<\":\"Disgust\",\n",
        "    u\"D:\":\"Sadness\",\n",
        "    u\"D8\":\"Great dismay\",\n",
        "    u\"D;\":\"Great dismay\",\n",
        "    u\"D=\":\"Great dismay\",\n",
        "    u\"DX\":\"Great dismay\",\n",
        "    u\":‑O\":\"Surprise\",\n",
        "    u\":O\":\"Surprise\",\n",
        "    u\":‑o\":\"Surprise\",\n",
        "    u\":o\":\"Surprise\",\n",
        "    u\":-0\":\"Shock\",\n",
        "    u\"8‑0\":\"Yawn\",\n",
        "    u\">:O\":\"Yawn\",\n",
        "    u\":-\\*\":\"Kiss\",\n",
        "    u\":\\*\":\"Kiss\",\n",
        "    u\":X\":\"Kiss\",\n",
        "    u\";‑\\)\":\"Wink or smirk\",\n",
        "    u\";\\)\":\"Wink or smirk\",\n",
        "    u\"\\*-\\)\":\"Wink or smirk\",\n",
        "    u\"\\*\\)\":\"Wink or smirk\",\n",
        "    u\";‑\\]\":\"Wink or smirk\",\n",
        "    u\";\\]\":\"Wink or smirk\",\n",
        "    u\";\\^\\)\":\"Wink or smirk\",\n",
        "    u\":‑,\":\"Wink or smirk\",\n",
        "    u\";D\":\"Wink or smirk\",\n",
        "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":‑\\|\":\"Straight face\",\n",
        "    u\":\\|\":\"Straight face\",\n",
        "    u\":$\":\"Embarrassed or blushing\",\n",
        "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\"O:‑\\)\":\"Angel, saint or innocent\",\n",
        "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
        "    u\"0:3\":\"Angel, saint or innocent\",\n",
        "    u\"0:‑\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
        "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
        "    u\">:‑\\)\":\"Evil or devilish\",\n",
        "    u\">:\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:‑\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:\\)\":\"Evil or devilish\",\n",
        "    u\"3:‑\\)\":\"Evil or devilish\",\n",
        "    u\"3:\\)\":\"Evil or devilish\",\n",
        "    u\">;\\)\":\"Evil or devilish\",\n",
        "    u\"\\|;‑\\)\":\"Cool\",\n",
        "    u\"\\|‑O\":\"Bored\",\n",
        "    u\":‑J\":\"Tongue-in-cheek\",\n",
        "    u\"#‑\\)\":\"Party all night\",\n",
        "    u\"%‑\\)\":\"Drunk or confused\",\n",
        "    u\"%\\)\":\"Drunk or confused\",\n",
        "    u\":-###..\":\"Being sick\",\n",
        "    u\":###..\":\"Being sick\",\n",
        "    u\"<:‑\\|\":\"Dump\",\n",
        "    u\"\\(>_<\\)\":\"Troubled\",\n",
        "    u\"\\(>_<\\)>\":\"Troubled\",\n",
        "    u\"\\(';'\\)\":\"Baby\",\n",
        "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
        "    u\"\\(\\^_-\\)\":\"Wink\",\n",
        "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
        "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
        "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
        "    u\"\\^_\\^\":\"Joyful\",\n",
        "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
        "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
        "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
        "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
        "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;_;\":\"Sad of Crying\",\n",
        "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
        "    u\";_;\":\"Sad or Crying\",\n",
        "    u\";-;\":\"Sad or Crying\",\n",
        "    u\";n;\":\"Sad or Crying\",\n",
        "    u\";;\":\"Sad or Crying\",\n",
        "    u\"Q\\.Q\":\"Sad or Crying\",\n",
        "    u\"T\\.T\":\"Sad or Crying\",\n",
        "    u\"QQ\":\"Sad or Crying\",\n",
        "    u\"Q_Q\":\"Sad or Crying\",\n",
        "    u\"\\(-\\.-\\)\":\"Shame\",\n",
        "    u\"\\(-_-\\)\":\"Shame\",\n",
        "    u\"\\(一一\\)\":\"Shame\",\n",
        "    u\"\\(；一_一\\)\":\"Shame\",\n",
        "    u\"\\(=_=\\)\":\"Tired\",\n",
        "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
        "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
        "    u\"=_\\^=\t\":\"cat\",\n",
        "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
        "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
        "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
        "    u\"\\(\\・\\・?\":\"Confusion\",\n",
        "    u\"\\(?_?\\)\":\"Confusion\",\n",
        "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
        "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
        "    u\"\\^/\\^\":\"Normal Laugh\",\n",
        "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
        "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
        "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
        "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
        "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
        "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
        "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
        "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
        "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
        "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
        "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
        "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
        "    u'\\(-\"-\\)':\"Worried\",\n",
        "    u\"\\(ーー;\\)\":\"Worried\",\n",
        "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
        "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
        "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
        "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
        "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
        "    u\":O o_O\":\"Surprised\",\n",
        "    u\"o_0\":\"Surprised\",\n",
        "    u\"o\\.O\":\"Surpised\",\n",
        "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
        "    u\"oO\":\"Surprised\",\n",
        "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
        "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "arEe0M-jGEwp",
        "outputId": "69ca8770-116a-4cc5-a02a-cd5076a00f13",
        "colab": {
          "referenced_widgets": [
            "b121ad5aacea4848b09c1550857d851c",
            "e05d8fb017b54d0b85309576e2cbd722",
            "f26a7af8c5d142248afe00a2cc117a53",
            "b19c9e1572064953a4abf5d68bfb31bc",
            "8f97cf10f7524e6aaccd354aaebfd624",
            "402ae51d1d684cc9923e4ac3aaff284d",
            "9646b4487b584d1bbf826e93c7b13cc3"
          ]
        }
      },
      "source": [
        "%%time\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import emoji\n",
        "tqdm().pandas()\n",
        "import dill\n",
        "# TODO: remove all the non-ascii characters\n",
        "# Hint, keep the data in place in pandas with df.[column].apply with a lambda function will be handy for such preprocessing steps.\n",
        "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
        "\n",
        "def remove_emoji_1(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "def remove_emoji_2(string):\n",
        "    return emoji.get_emoji_regexp().sub(u'', string)\n",
        "\n",
        "def remove_emoticons(text):\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
        "    return emoticon_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "\n",
        "df_rm_emo = df_en.copy()\n",
        "df_rm_emo.text_x=df_rm_emo.text_x.progress_apply(lambda x : remove_emoji_1(x))\n",
        "df_rm_emo.text_y=df_rm_emo.text_y.progress_apply(lambda x : remove_emoji_1(x))\n",
        "\n",
        "\n",
        "df_rm_emo.text_x=df_rm_emo.text_x.progress_apply(lambda x : remove_emoji_2(x))\n",
        "df_rm_emo.text_y=df_rm_emo.text_y.progress_apply(lambda x : remove_emoji_2(x))\n",
        "\n",
        "df_rm_emo.text_x=df_rm_emo.text_x.progress_apply(lambda x : remove_emoticons(x))\n",
        "df_rm_emo.text_y=df_rm_emo.text_y.progress_apply(lambda x : remove_emoticons(x))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b121ad5aacea4848b09c1550857d851c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e05d8fb017b54d0b85309576e2cbd722",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1152652), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f26a7af8c5d142248afe00a2cc117a53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1152652), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b19c9e1572064953a4abf5d68bfb31bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1152652), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f97cf10f7524e6aaccd354aaebfd624",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1152652), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "402ae51d1d684cc9923e4ac3aaff284d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1152652), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9646b4487b584d1bbf826e93c7b13cc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1152652), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Wall time: 20min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4R3z_JwZoye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_rm_emo.to_csv(DATA_FOLDER + 'tweets_conversations_en_rm_emo.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU2PECdYZoyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_rm_emo = pd.read_csv(DATA_FOLDER + 'tweets_conversations_en_rm_emo.csv', lineterminator='\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1efdae4f-8ff9-4242-be79-06916771da66",
        "_uuid": "44f124ae8898978cf5c1e571da976888f8229894",
        "colab_type": "code",
        "id": "oNdwUbVrGEws",
        "outputId": "2bc07129-1687-483f-e8ea-30a14174667a",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "# TODO: Replace anonymized screen names with common token @__sn__\n",
        "df_rp_name = df_rm_emo.copy()\n",
        "df_rp_name['text_x'] = df_rp_name['text_x'].str.replace('@\\d+', '@__sn__')\n",
        "df_rp_name['text_y'] = df_rp_name['text_y'].str.replace('@\\d+', '@__sn__')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 2.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPa513oAZoyj",
        "colab_type": "code",
        "outputId": "959b7661-3e72-493e-a0f3-f6d7a18d9dd5",
        "colab": {}
      },
      "source": [
        "df_rp_name.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_x</th>\n",
              "      <th>text_y</th>\n",
              "      <th>author_id_y\\r\\r\\r</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>Stuck on a @VirginAtlantic plane at Gatwick be...</td>\n",
              "      <td>@__sn__ Sorry to hear that Brian. Which flight...</td>\n",
              "      <td>VirginAtlantic\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>@__sn__: There is a payment I made to Green Mo...</td>\n",
              "      <td>@__sn__ Pls click below to DM your name/ZIP/ph...</td>\n",
              "      <td>BofA_Help\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>@AppleSupport fix this restarting problem plea...</td>\n",
              "      <td>@__sn__ Send us a DM and tell us what issues y...</td>\n",
              "      <td>AppleSupport\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>@GWRHelp Will you consider pre booked seats on...</td>\n",
              "      <td>@__sn__ Hi there. Can you please explain what ...</td>\n",
              "      <td>GWRHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>@idea_cares why is it compulsory to recharge w...</td>\n",
              "      <td>@__sn__ We would request you to share your ide...</td>\n",
              "      <td>idea_cares\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>@UPSHelp hi I have a package coming tomorrow b...</td>\n",
              "      <td>@__sn__ I can arrange to have this matter look...</td>\n",
              "      <td>UPSHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>@GWRHelp Thanks. That's a shame. We were looki...</td>\n",
              "      <td>@__sn__ It doesn't operate at weekends. You ca...</td>\n",
              "      <td>GWRHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>thanks @__sn__ for fucking my signed first edi...</td>\n",
              "      <td>@__sn__ Sorry that this happened. We can start...</td>\n",
              "      <td>UPSHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>@Ask_Spectrum 100mbps for $69 +$4 modem orEver...</td>\n",
              "      <td>@__sn__ I’m sorry, we don’t handle this kind o...</td>\n",
              "      <td>Ask_Spectrum\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>A @__sn__ MANAGER literally just angrily argue...</td>\n",
              "      <td>@__sn__ I can see you're upset, Mark. Pls DM u...</td>\n",
              "      <td>Ask_WellsFargo\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>I have an iphone 7 plus and iOS 11 has destroy...</td>\n",
              "      <td>@__sn__ We'll be happy to look into this. DM u...</td>\n",
              "      <td>AppleSupport\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>Always proud to work for Gary Kelly of @Southw...</td>\n",
              "      <td>@__sn__ He walks the walk, Mark! Thanks for th...</td>\n",
              "      <td>SouthwestAir\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>@Postmates_Help  Hey I got charged $114 twice ...</td>\n",
              "      <td>@__sn__ Hi there. If the charges read TEMP AUT...</td>\n",
              "      <td>Postmates_Help\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>@AppleSupport Was on a scheduled call an hour ...</td>\n",
              "      <td>@__sn__ We'd be happy to look into this with y...</td>\n",
              "      <td>AppleSupport\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>I JUST WANT TO GO HOME BUT MY @__sn__ APP WONT...</td>\n",
              "      <td>@__sn__ Here to help! Send us a note here; htt...</td>\n",
              "      <td>Uber_Support\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>@CenturyLinkHelp Just curious, I've seen a lot...</td>\n",
              "      <td>@__sn__ I would be happy to verify with your a...</td>\n",
              "      <td>CenturyLinkHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>@Uber_Support About that... he found his phone...</td>\n",
              "      <td>@__sn__ Sounds good! Let us know if we can hel...</td>\n",
              "      <td>Uber_Support\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>@AlaskaAir Hopefully I’m not on their flight a...</td>\n",
              "      <td>@__sn__ Happy Thanksgiving, have a great trip....</td>\n",
              "      <td>AlaskaAir\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>@AmericanAir Well done... I was worried about ...</td>\n",
              "      <td>@__sn__ We've got you covered, Ben! See you on...</td>\n",
              "      <td>AmericanAir\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>@XboxSupport Can you tell me if this will ever...</td>\n",
              "      <td>@__sn__ HI there! Let's go ahead and remove: h...</td>\n",
              "      <td>XboxSupport\\r\\r\\r</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               text_x  \\\n",
              "0   Stuck on a @VirginAtlantic plane at Gatwick be...   \n",
              "1   @__sn__: There is a payment I made to Green Mo...   \n",
              "2   @AppleSupport fix this restarting problem plea...   \n",
              "3   @GWRHelp Will you consider pre booked seats on...   \n",
              "4   @idea_cares why is it compulsory to recharge w...   \n",
              "5   @UPSHelp hi I have a package coming tomorrow b...   \n",
              "6   @GWRHelp Thanks. That's a shame. We were looki...   \n",
              "7   thanks @__sn__ for fucking my signed first edi...   \n",
              "8   @Ask_Spectrum 100mbps for $69 +$4 modem orEver...   \n",
              "9   A @__sn__ MANAGER literally just angrily argue...   \n",
              "10  I have an iphone 7 plus and iOS 11 has destroy...   \n",
              "11  Always proud to work for Gary Kelly of @Southw...   \n",
              "12  @Postmates_Help  Hey I got charged $114 twice ...   \n",
              "13  @AppleSupport Was on a scheduled call an hour ...   \n",
              "14  I JUST WANT TO GO HOME BUT MY @__sn__ APP WONT...   \n",
              "15  @CenturyLinkHelp Just curious, I've seen a lot...   \n",
              "16  @Uber_Support About that... he found his phone...   \n",
              "17  @AlaskaAir Hopefully I’m not on their flight a...   \n",
              "18  @AmericanAir Well done... I was worried about ...   \n",
              "19  @XboxSupport Can you tell me if this will ever...   \n",
              "\n",
              "                                               text_y      author_id_y\\r\\r\\r  \n",
              "0   @__sn__ Sorry to hear that Brian. Which flight...   VirginAtlantic\\r\\r\\r  \n",
              "1   @__sn__ Pls click below to DM your name/ZIP/ph...        BofA_Help\\r\\r\\r  \n",
              "2   @__sn__ Send us a DM and tell us what issues y...     AppleSupport\\r\\r\\r  \n",
              "3   @__sn__ Hi there. Can you please explain what ...          GWRHelp\\r\\r\\r  \n",
              "4   @__sn__ We would request you to share your ide...       idea_cares\\r\\r\\r  \n",
              "5   @__sn__ I can arrange to have this matter look...          UPSHelp\\r\\r\\r  \n",
              "6   @__sn__ It doesn't operate at weekends. You ca...          GWRHelp\\r\\r\\r  \n",
              "7   @__sn__ Sorry that this happened. We can start...          UPSHelp\\r\\r\\r  \n",
              "8   @__sn__ I’m sorry, we don’t handle this kind o...     Ask_Spectrum\\r\\r\\r  \n",
              "9   @__sn__ I can see you're upset, Mark. Pls DM u...   Ask_WellsFargo\\r\\r\\r  \n",
              "10  @__sn__ We'll be happy to look into this. DM u...     AppleSupport\\r\\r\\r  \n",
              "11  @__sn__ He walks the walk, Mark! Thanks for th...     SouthwestAir\\r\\r\\r  \n",
              "12  @__sn__ Hi there. If the charges read TEMP AUT...   Postmates_Help\\r\\r\\r  \n",
              "13  @__sn__ We'd be happy to look into this with y...     AppleSupport\\r\\r\\r  \n",
              "14  @__sn__ Here to help! Send us a note here; htt...     Uber_Support\\r\\r\\r  \n",
              "15  @__sn__ I would be happy to verify with your a...  CenturyLinkHelp\\r\\r\\r  \n",
              "16  @__sn__ Sounds good! Let us know if we can hel...     Uber_Support\\r\\r\\r  \n",
              "17  @__sn__ Happy Thanksgiving, have a great trip....        AlaskaAir\\r\\r\\r  \n",
              "18  @__sn__ We've got you covered, Ben! See you on...      AmericanAir\\r\\r\\r  \n",
              "19  @__sn__ HI there! Let's go ahead and remove: h...      XboxSupport\\r\\r\\r  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVaKX-UZOp1T",
        "outputId": "767bd6d5-6932-43d6-ccf8-4be9873dbde4",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# TODO: Replace URLs with <URL>\n",
        "# Hint: you might need to use some simple regular expression\n",
        "import re\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return url_pattern.sub(r'<URL>', text)\n",
        "\n",
        "df_rp_url = df_rp_name.copy()\n",
        "df_rp_url['text_x'] = df_rp_url['text_x'].str.replace('https?://\\S+|www\\.\\S+', '<URL>')\n",
        "df_rp_url['text_y'] = df_rp_url['text_y'].str.replace('https?://\\S+|www\\.\\S+', '<URL>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 4.19 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPEl1rCDZoym",
        "colab_type": "code",
        "outputId": "9d984b69-6142-4d27-e979-5ddb27055d3a",
        "colab": {}
      },
      "source": [
        "df_rp_url.head(30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_x</th>\n",
              "      <th>text_y</th>\n",
              "      <th>author_id_y\\r\\r\\r</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>Stuck on a @VirginAtlantic plane at Gatwick be...</td>\n",
              "      <td>@__sn__ Sorry to hear that Brian. Which flight...</td>\n",
              "      <td>VirginAtlantic\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>@__sn__: There is a payment I made to Green Mo...</td>\n",
              "      <td>@__sn__ Pls click below to DM your name/ZIP/ph...</td>\n",
              "      <td>BofA_Help\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>@AppleSupport fix this restarting problem plea...</td>\n",
              "      <td>@__sn__ Send us a DM and tell us what issues y...</td>\n",
              "      <td>AppleSupport\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>@GWRHelp Will you consider pre booked seats on...</td>\n",
              "      <td>@__sn__ Hi there. Can you please explain what ...</td>\n",
              "      <td>GWRHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>@idea_cares why is it compulsory to recharge w...</td>\n",
              "      <td>@__sn__ We would request you to share your ide...</td>\n",
              "      <td>idea_cares\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>@UPSHelp hi I have a package coming tomorrow b...</td>\n",
              "      <td>@__sn__ I can arrange to have this matter look...</td>\n",
              "      <td>UPSHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>@GWRHelp Thanks. That's a shame. We were looki...</td>\n",
              "      <td>@__sn__ It doesn't operate at weekends. You ca...</td>\n",
              "      <td>GWRHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>thanks @__sn__ for fucking my signed first edi...</td>\n",
              "      <td>@__sn__ Sorry that this happened. We can start...</td>\n",
              "      <td>UPSHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>@Ask_Spectrum 100mbps for $69 +$4 modem orEver...</td>\n",
              "      <td>@__sn__ I’m sorry, we don’t handle this kind o...</td>\n",
              "      <td>Ask_Spectrum\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>A @__sn__ MANAGER literally just angrily argue...</td>\n",
              "      <td>@__sn__ I can see you're upset, Mark. Pls DM u...</td>\n",
              "      <td>Ask_WellsFargo\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>I have an iphone 7 plus and iOS 11 has destroy...</td>\n",
              "      <td>@__sn__ We'll be happy to look into this. DM u...</td>\n",
              "      <td>AppleSupport\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>Always proud to work for Gary Kelly of @Southw...</td>\n",
              "      <td>@__sn__ He walks the walk, Mark! Thanks for th...</td>\n",
              "      <td>SouthwestAir\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>@Postmates_Help  Hey I got charged $114 twice ...</td>\n",
              "      <td>@__sn__ Hi there. If the charges read TEMP AUT...</td>\n",
              "      <td>Postmates_Help\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>@AppleSupport Was on a scheduled call an hour ...</td>\n",
              "      <td>@__sn__ We'd be happy to look into this with y...</td>\n",
              "      <td>AppleSupport\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>I JUST WANT TO GO HOME BUT MY @__sn__ APP WONT...</td>\n",
              "      <td>@__sn__ Here to help! Send us a note here; htt...</td>\n",
              "      <td>Uber_Support\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>@CenturyLinkHelp Just curious, I've seen a lot...</td>\n",
              "      <td>@__sn__ I would be happy to verify with your a...</td>\n",
              "      <td>CenturyLinkHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>@Uber_Support About that... he found his phone...</td>\n",
              "      <td>@__sn__ Sounds good! Let us know if we can hel...</td>\n",
              "      <td>Uber_Support\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>@AlaskaAir Hopefully I’m not on their flight a...</td>\n",
              "      <td>@__sn__ Happy Thanksgiving, have a great trip....</td>\n",
              "      <td>AlaskaAir\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>@AmericanAir Well done... I was worried about ...</td>\n",
              "      <td>@__sn__ We've got you covered, Ben! See you on...</td>\n",
              "      <td>AmericanAir\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>@XboxSupport Can you tell me if this will ever...</td>\n",
              "      <td>@__sn__ HI there! Let's go ahead and remove: h...</td>\n",
              "      <td>XboxSupport\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>@Uber_Support what do I do if I scheduled a ri...</td>\n",
              "      <td>@__sn__ Here to help!  Please follow the link ...</td>\n",
              "      <td>Uber_Support\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>@AmericanAir second time my flight got diverte...</td>\n",
              "      <td>@__sn__ We know missing a connection is upsett...</td>\n",
              "      <td>AmericanAir\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>@__sn__ your technical service team needs to l...</td>\n",
              "      <td>@__sn__ Hi Deepak, Please accept our sincere a...</td>\n",
              "      <td>DellCares\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>Hey @__sn__, my laptop battery doesn't work fo...</td>\n",
              "      <td>@__sn__ I'm sorry to know about the trouble yo...</td>\n",
              "      <td>AmazonHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>Winnipeg totally does not need #Uber waiting 4...</td>\n",
              "      <td>@__sn__ We're sorry to hear this was your expe...</td>\n",
              "      <td>Uber_Support\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>@AlaskaAir I already have this flight, it allo...</td>\n",
              "      <td>@__sn__ Hey Shane in order to add a Gold Guest...</td>\n",
              "      <td>AlaskaAir\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>This morning is just right: 30% coffee and 90%...</td>\n",
              "      <td>@__sn__ Delivered with 200% likability</td>\n",
              "      <td>O2\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>@AskPayPal I'll do that but it was definitely ...</td>\n",
              "      <td>@__sn__ Hi Michael, do you mind also sending a...</td>\n",
              "      <td>AskPayPal\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>@nationalrailenq hi, going from Salisbury to A...</td>\n",
              "      <td>@__sn__ Hi there, yes this is correct, buses w...</td>\n",
              "      <td>nationalrailenq\\r\\r\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>@AmazonHelp I ordered a bookshelf, prime, on S...</td>\n",
              "      <td>@__sn__ It sounds like you were refunded the s...</td>\n",
              "      <td>AmazonHelp\\r\\r\\r</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               text_x  \\\n",
              "0   Stuck on a @VirginAtlantic plane at Gatwick be...   \n",
              "1   @__sn__: There is a payment I made to Green Mo...   \n",
              "2   @AppleSupport fix this restarting problem plea...   \n",
              "3   @GWRHelp Will you consider pre booked seats on...   \n",
              "4   @idea_cares why is it compulsory to recharge w...   \n",
              "5   @UPSHelp hi I have a package coming tomorrow b...   \n",
              "6   @GWRHelp Thanks. That's a shame. We were looki...   \n",
              "7   thanks @__sn__ for fucking my signed first edi...   \n",
              "8   @Ask_Spectrum 100mbps for $69 +$4 modem orEver...   \n",
              "9   A @__sn__ MANAGER literally just angrily argue...   \n",
              "10  I have an iphone 7 plus and iOS 11 has destroy...   \n",
              "11  Always proud to work for Gary Kelly of @Southw...   \n",
              "12  @Postmates_Help  Hey I got charged $114 twice ...   \n",
              "13  @AppleSupport Was on a scheduled call an hour ...   \n",
              "14  I JUST WANT TO GO HOME BUT MY @__sn__ APP WONT...   \n",
              "15  @CenturyLinkHelp Just curious, I've seen a lot...   \n",
              "16  @Uber_Support About that... he found his phone...   \n",
              "17  @AlaskaAir Hopefully I’m not on their flight a...   \n",
              "18  @AmericanAir Well done... I was worried about ...   \n",
              "19  @XboxSupport Can you tell me if this will ever...   \n",
              "20  @Uber_Support what do I do if I scheduled a ri...   \n",
              "21  @AmericanAir second time my flight got diverte...   \n",
              "22  @__sn__ your technical service team needs to l...   \n",
              "23  Hey @__sn__, my laptop battery doesn't work fo...   \n",
              "24  Winnipeg totally does not need #Uber waiting 4...   \n",
              "25  @AlaskaAir I already have this flight, it allo...   \n",
              "26  This morning is just right: 30% coffee and 90%...   \n",
              "27  @AskPayPal I'll do that but it was definitely ...   \n",
              "28  @nationalrailenq hi, going from Salisbury to A...   \n",
              "29  @AmazonHelp I ordered a bookshelf, prime, on S...   \n",
              "\n",
              "                                               text_y      author_id_y\\r\\r\\r  \n",
              "0   @__sn__ Sorry to hear that Brian. Which flight...   VirginAtlantic\\r\\r\\r  \n",
              "1   @__sn__ Pls click below to DM your name/ZIP/ph...        BofA_Help\\r\\r\\r  \n",
              "2   @__sn__ Send us a DM and tell us what issues y...     AppleSupport\\r\\r\\r  \n",
              "3   @__sn__ Hi there. Can you please explain what ...          GWRHelp\\r\\r\\r  \n",
              "4   @__sn__ We would request you to share your ide...       idea_cares\\r\\r\\r  \n",
              "5   @__sn__ I can arrange to have this matter look...          UPSHelp\\r\\r\\r  \n",
              "6   @__sn__ It doesn't operate at weekends. You ca...          GWRHelp\\r\\r\\r  \n",
              "7   @__sn__ Sorry that this happened. We can start...          UPSHelp\\r\\r\\r  \n",
              "8   @__sn__ I’m sorry, we don’t handle this kind o...     Ask_Spectrum\\r\\r\\r  \n",
              "9   @__sn__ I can see you're upset, Mark. Pls DM u...   Ask_WellsFargo\\r\\r\\r  \n",
              "10  @__sn__ We'll be happy to look into this. DM u...     AppleSupport\\r\\r\\r  \n",
              "11  @__sn__ He walks the walk, Mark! Thanks for th...     SouthwestAir\\r\\r\\r  \n",
              "12  @__sn__ Hi there. If the charges read TEMP AUT...   Postmates_Help\\r\\r\\r  \n",
              "13  @__sn__ We'd be happy to look into this with y...     AppleSupport\\r\\r\\r  \n",
              "14  @__sn__ Here to help! Send us a note here; htt...     Uber_Support\\r\\r\\r  \n",
              "15  @__sn__ I would be happy to verify with your a...  CenturyLinkHelp\\r\\r\\r  \n",
              "16  @__sn__ Sounds good! Let us know if we can hel...     Uber_Support\\r\\r\\r  \n",
              "17  @__sn__ Happy Thanksgiving, have a great trip....        AlaskaAir\\r\\r\\r  \n",
              "18  @__sn__ We've got you covered, Ben! See you on...      AmericanAir\\r\\r\\r  \n",
              "19  @__sn__ HI there! Let's go ahead and remove: h...      XboxSupport\\r\\r\\r  \n",
              "20  @__sn__ Here to help!  Please follow the link ...     Uber_Support\\r\\r\\r  \n",
              "21  @__sn__ We know missing a connection is upsett...      AmericanAir\\r\\r\\r  \n",
              "22  @__sn__ Hi Deepak, Please accept our sincere a...        DellCares\\r\\r\\r  \n",
              "23  @__sn__ I'm sorry to know about the trouble yo...       AmazonHelp\\r\\r\\r  \n",
              "24  @__sn__ We're sorry to hear this was your expe...     Uber_Support\\r\\r\\r  \n",
              "25  @__sn__ Hey Shane in order to add a Gold Guest...        AlaskaAir\\r\\r\\r  \n",
              "26             @__sn__ Delivered with 200% likability               O2\\r\\r\\r  \n",
              "27  @__sn__ Hi Michael, do you mind also sending a...        AskPayPal\\r\\r\\r  \n",
              "28  @__sn__ Hi there, yes this is correct, buses w...  nationalrailenq\\r\\r\\r  \n",
              "29  @__sn__ It sounds like you were refunded the s...       AmazonHelp\\r\\r\\r  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tTnAFrw2Op1W",
        "colab": {}
      },
      "source": [
        "# TODO: Remove hashtags\n",
        "df_rp_ht = df_rp_url.copy()\n",
        "df_rp_ht['text_x'] = df_rp_ht['text_x'].str.replace('#\\S+', '')\n",
        "df_rp_ht['text_y'] = df_rp_ht['text_y'].str.replace('#\\S+', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EkPCfyT_Op1Y",
        "colab": {}
      },
      "source": [
        "# TODO: Remove duplicate spaces\n",
        "df_rp_ds = df_rp_ht.copy()\n",
        "df_rp_ds['text_x'] = df_rp_ds['text_x'].str.replace('\\s\\s+', ' ')\n",
        "df_rp_ds['text_y'] = df_rp_ds['text_y'].str.replace('\\s\\s+', ' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mELuPWipOp1e",
        "colab": {}
      },
      "source": [
        "# TODO: lowercase all text\n",
        "df_rp_lw = df_rp_ds.copy()\n",
        "df_rp_lw['text_x'] = df_rp_lw['text_x'].str.lower()\n",
        "df_rp_lw['text_y'] = df_rp_lw['text_y'].str.lower()\n",
        "df_rp_lw['author_id_y\\r\\r\\r'] = df_rp_lw['author_id_y\\r\\r\\r'].str.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bgTTKJsEGEwr",
        "colab": {}
      },
      "source": [
        "df_rp_lw.to_csv(DATA_FOLDER + 'tweets_conversations_en_clean.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MzRAdTbAGEwz"
      },
      "source": [
        "#### 2. Tokenization\n",
        "\n",
        "In natural language generation (chatbot is an example), it's very important that you keep the vocabulary size relatively small. At every time step, you have a huge `softmax` over the whole vocabulary to decide what's the next word you gonna say. If you have 100k vocabulary, it's gonna be a hard decision. \n",
        "\n",
        "To this end, we'll use a trick called subword-level tokenization, which we'll tokenize the word `proactive` into `pro-`, `act` and `-ive`. There's [a library by Google](https://github.com/google/sentencepiece) which does this for us. You can find the Python library and use it to tokenize. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GBLwWJysOp1n",
        "outputId": "6f152133-f88c-4e51-cd18-31e1a317f939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "# TODO: train sentencepiece tokenizer\n",
        "df_en = pd.read_csv(DATA_FOLDER + 'tweets_conversations_en_clean.csv', lineterminator='\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6eb81bdef084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'tweets_conversations_en_clean.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a3nXs0I2Op1r",
        "colab": {}
      },
      "source": [
        "# TODO: prepare the data to train sentencepiece tokenizer. \n",
        "# Read the documentation from the link above.\n",
        "# write the input data to tweets.temp to train SentencePiece tokenizer.\n",
        "with open(DATA_FOLDER +'tweets.temp', mode='w', encoding=\"utf-8\") as f:\n",
        "    for sentence in df_en['text_x']:\n",
        "        f.write(str(sentence) + '\\n')\n",
        "    for sentence in df_en['text_y']:\n",
        "        f.write(str(sentence)+ '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "as3XpWmxOp1u",
        "outputId": "dad751aa-f255-4c36-d771-4aaaceca62a9",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# train sentence piece tokenizer\n",
        "import sentencepiece as spm\n",
        "spm.SentencePieceTrainer.train('--input=%s --model_prefix=twitter --vocab_size=%d --bos_id=%d --eos_id=%d --pad_id=%d --hard_vocab_limit=false' %('tweets.temp', VOCAB_SIZE, START, END, PAD))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 6min 24s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yV31ISLNOp1x",
        "outputId": "2b0c2e86-8262-4f2d-a933-d63ed8c77349",
        "colab": {}
      },
      "source": [
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "# sp.Load(\"models/tokenization/twitter.model\")\n",
        "\n",
        "sp.Load(\"twitter.model\")\n",
        "# Encode the sentence\n",
        "ids = sp.EncodeAsIds(\"this is a test\")\n",
        "print('EncodeAsIds',ids)\n",
        "# Decode the sentence\n",
        "sp.DecodeIds(ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncodeAsIds [25, 31, 14, 1670]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a test'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bMfSH_c_Op1z",
        "outputId": "1f619ebe-b7d6-493b-fe3d-77005ddb3a22",
        "scrolled": false,
        "colab": {
          "referenced_widgets": [
            "bdef9bed0d3843f8b02f6c8575b1dec4",
            "80d20e0c9bee45288822fe58efbda7e2",
            "3ec5e3fa01bc48fab7225a8907085fc2"
          ]
        }
      },
      "source": [
        "%%time\n",
        "# TODO: Transform the input data to token ids with the tokenizer\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "tqdm().pandas()\n",
        "\n",
        "df_en['tokens_x'] = df_en['text_x'].progress_apply(lambda x : sp.EncodeAsIds(str(x)))\n",
        "df_en['tokens_y'] = df_en['text_y'].progress_apply(lambda x : [START] + sp.EncodeAsIds(str(x)) + [END])\n",
        "df_en.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdef9bed0d3843f8b02f6c8575b1dec4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80d20e0c9bee45288822fe58efbda7e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1152652), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ec5e3fa01bc48fab7225a8907085fc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1152652), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Wall time: 2min 19s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_x</th>\n",
              "      <th>text_y</th>\n",
              "      <th>author_id_y\\r\\r\\r\\r</th>\n",
              "      <th>tokens_x</th>\n",
              "      <th>tokens_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>stuck on a @virginatlantic plane at gatwick be...</td>\n",
              "      <td>@__sn__ sorry to hear that brian. which flight...</td>\n",
              "      <td>virginatlantic\\r\\r\\r\\r</td>\n",
              "      <td>[1559, 28, 14, 5, 921, 69, 108, 2108, 7, 2165,...</td>\n",
              "      <td>[2, 5, 894, 8, 70, 894, 102, 6, 160, 45, 310, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>@__sn__: there is a payment i made to green mo...</td>\n",
              "      <td>@__sn__ pls click below to dm your name/zip/ph...</td>\n",
              "      <td>bofa_help\\r\\r\\r\\r</td>\n",
              "      <td>[5, 894, 8, 70, 894, 59, 68, 31, 14, 747, 163,...</td>\n",
              "      <td>[2, 5, 894, 8, 70, 894, 1167, 749, 1018, 6, 17...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>@applesupport fix this restarting problem plea...</td>\n",
              "      <td>@__sn__ send us a dm and tell us what issues y...</td>\n",
              "      <td>applesupport\\r\\r\\r\\r</td>\n",
              "      <td>[5, 3440, 571, 316, 25, 1985, 391, 74, 652, 14...</td>\n",
              "      <td>[2, 5, 894, 8, 70, 894, 144, 29, 14, 1736, 16,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>@gwrhelp will you consider pre booked seats on...</td>\n",
              "      <td>@__sn__ hi there. can you please explain what ...</td>\n",
              "      <td>gwrhelp\\r\\r\\r\\r</td>\n",
              "      <td>[5, 108, 137, 69, 1609, 82, 11, 989, 669, 1292...</td>\n",
              "      <td>[2, 5, 894, 8, 70, 894, 498, 68, 4, 27, 11, 74...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>@idea_cares why is it compulsory to recharge w...</td>\n",
              "      <td>@__sn__ we would request you to share your ide...</td>\n",
              "      <td>idea_cares\\r\\r\\r\\r</td>\n",
              "      <td>[5, 812, 76, 813, 214, 31, 30, 1155, 630, 8, 2...</td>\n",
              "      <td>[2, 5, 894, 8, 70, 894, 35, 171, 542, 11, 6, 4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              text_x  \\\n",
              "0  stuck on a @virginatlantic plane at gatwick be...   \n",
              "1  @__sn__: there is a payment i made to green mo...   \n",
              "2  @applesupport fix this restarting problem plea...   \n",
              "3  @gwrhelp will you consider pre booked seats on...   \n",
              "4  @idea_cares why is it compulsory to recharge w...   \n",
              "\n",
              "                                              text_y     author_id_y\\r\\r\\r\\r  \\\n",
              "0  @__sn__ sorry to hear that brian. which flight...  virginatlantic\\r\\r\\r\\r   \n",
              "1  @__sn__ pls click below to dm your name/zip/ph...       bofa_help\\r\\r\\r\\r   \n",
              "2  @__sn__ send us a dm and tell us what issues y...    applesupport\\r\\r\\r\\r   \n",
              "3  @__sn__ hi there. can you please explain what ...         gwrhelp\\r\\r\\r\\r   \n",
              "4  @__sn__ we would request you to share your ide...      idea_cares\\r\\r\\r\\r   \n",
              "\n",
              "                                            tokens_x  \\\n",
              "0  [1559, 28, 14, 5, 921, 69, 108, 2108, 7, 2165,...   \n",
              "1  [5, 894, 8, 70, 894, 59, 68, 31, 14, 747, 163,...   \n",
              "2  [5, 3440, 571, 316, 25, 1985, 391, 74, 652, 14...   \n",
              "3  [5, 108, 137, 69, 1609, 82, 11, 989, 669, 1292...   \n",
              "4  [5, 812, 76, 813, 214, 31, 30, 1155, 630, 8, 2...   \n",
              "\n",
              "                                            tokens_y  \n",
              "0  [2, 5, 894, 8, 70, 894, 102, 6, 160, 45, 310, ...  \n",
              "1  [2, 5, 894, 8, 70, 894, 1167, 749, 1018, 6, 17...  \n",
              "2  [2, 5, 894, 8, 70, 894, 144, 29, 14, 1736, 16,...  \n",
              "3  [2, 5, 894, 8, 70, 894, 498, 68, 4, 27, 11, 74...  \n",
              "4  [2, 5, 894, 8, 70, 894, 35, 171, 542, 11, 6, 4...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N5lapBaZoy6",
        "colab_type": "code",
        "outputId": "a8438353-3cd5-4375-fa18-9ad2903a8a77",
        "colab": {}
      },
      "source": [
        "df_en.rename(columns={'author_id_y\\r\\r\\r\\r' : 'author_id_y'},inplace=True)\n",
        "df_en['author_id_y'] = df_en['author_id_y'].str.replace('\\r\\r\\r\\r', '')\n",
        "df_en.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_x</th>\n",
              "      <th>text_y</th>\n",
              "      <th>author_id_y</th>\n",
              "      <th>tokens_x</th>\n",
              "      <th>tokens_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>stuck on a @virginatlantic plane at gatwick be...</td>\n",
              "      <td>@__sn__ sorry to hear that brian. which flight...</td>\n",
              "      <td>virginatlantic</td>\n",
              "      <td>[1559, 28, 14, 5, 921, 69, 108, 2108, 7, 2165,...</td>\n",
              "      <td>[2, 5, 894, 8, 70, 894, 102, 6, 160, 45, 310, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>@__sn__: there is a payment i made to green mo...</td>\n",
              "      <td>@__sn__ pls click below to dm your name/zip/ph...</td>\n",
              "      <td>bofa_help</td>\n",
              "      <td>[5, 894, 8, 70, 894, 59, 68, 31, 14, 747, 163,...</td>\n",
              "      <td>[2, 5, 894, 8, 70, 894, 1167, 749, 1018, 6, 17...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>@applesupport fix this restarting problem plea...</td>\n",
              "      <td>@__sn__ send us a dm and tell us what issues y...</td>\n",
              "      <td>applesupport</td>\n",
              "      <td>[5, 3440, 571, 316, 25, 1985, 391, 74, 652, 14...</td>\n",
              "      <td>[2, 5, 894, 8, 70, 894, 144, 29, 14, 1736, 16,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>@gwrhelp will you consider pre booked seats on...</td>\n",
              "      <td>@__sn__ hi there. can you please explain what ...</td>\n",
              "      <td>gwrhelp</td>\n",
              "      <td>[5, 108, 137, 69, 1609, 82, 11, 989, 669, 1292...</td>\n",
              "      <td>[2, 5, 894, 8, 70, 894, 498, 68, 4, 27, 11, 74...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>@idea_cares why is it compulsory to recharge w...</td>\n",
              "      <td>@__sn__ we would request you to share your ide...</td>\n",
              "      <td>idea_cares</td>\n",
              "      <td>[5, 812, 76, 813, 214, 31, 30, 1155, 630, 8, 2...</td>\n",
              "      <td>[2, 5, 894, 8, 70, 894, 35, 171, 542, 11, 6, 4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              text_x  \\\n",
              "0  stuck on a @virginatlantic plane at gatwick be...   \n",
              "1  @__sn__: there is a payment i made to green mo...   \n",
              "2  @applesupport fix this restarting problem plea...   \n",
              "3  @gwrhelp will you consider pre booked seats on...   \n",
              "4  @idea_cares why is it compulsory to recharge w...   \n",
              "\n",
              "                                              text_y     author_id_y  \\\n",
              "0  @__sn__ sorry to hear that brian. which flight...  virginatlantic   \n",
              "1  @__sn__ pls click below to dm your name/zip/ph...       bofa_help   \n",
              "2  @__sn__ send us a dm and tell us what issues y...    applesupport   \n",
              "3  @__sn__ hi there. can you please explain what ...         gwrhelp   \n",
              "4  @__sn__ we would request you to share your ide...      idea_cares   \n",
              "\n",
              "                                            tokens_x  \\\n",
              "0  [1559, 28, 14, 5, 921, 69, 108, 2108, 7, 2165,...   \n",
              "1  [5, 894, 8, 70, 894, 59, 68, 31, 14, 747, 163,...   \n",
              "2  [5, 3440, 571, 316, 25, 1985, 391, 74, 652, 14...   \n",
              "3  [5, 108, 137, 69, 1609, 82, 11, 989, 669, 1292...   \n",
              "4  [5, 812, 76, 813, 214, 31, 30, 1155, 630, 8, 2...   \n",
              "\n",
              "                                            tokens_y  \n",
              "0  [2, 5, 894, 8, 70, 894, 102, 6, 160, 45, 310, ...  \n",
              "1  [2, 5, 894, 8, 70, 894, 1167, 749, 1018, 6, 17...  \n",
              "2  [2, 5, 894, 8, 70, 894, 144, 29, 14, 1736, 16,...  \n",
              "3  [2, 5, 894, 8, 70, 894, 498, 68, 4, 27, 11, 74...  \n",
              "4  [2, 5, 894, 8, 70, 894, 35, 171, 542, 11, 6, 4...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnfvVrWKOp11",
        "outputId": "6cfcda65-450e-4219-fee2-ce2b62dc3d9c",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# experiment with the sequence length. Seems like max_seq_len of 50 is enough\n",
        "df_en['token_count'] = df_en['tokens_x'].apply(lambda x: len(x))\n",
        "token_count = df_en['token_count'].values\n",
        "\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "num_bins = 100\n",
        "n, bins, patches = plt.hist(token_count, num_bins, facecolor='blue', alpha=0.5, range=[0,100])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVQjDt05Zoy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_en.to_csv(DATA_FOLDER + 'tweets_conversations_en_clean_tokenize.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "edebf7ab-0495-4e38-9900-7a9652cdb2d0",
        "_uuid": "f9f842020ecb321805c9032d20b01f1ffe6f5885",
        "colab_type": "text",
        "id": "7mOYOxVLGExP"
      },
      "source": [
        "### Train / Test Split\n",
        "Here, we split our data into training and test sets.  For simplicity, we use a random split, which may result in different distributions between the training and test set, but we won't worry about that for this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JFiAhVXFOp2C",
        "colab": {}
      },
      "source": [
        "#TODO: split the train & test set\n",
        "x = df_en['tokens_x']\n",
        "y = df_en['tokens_y']\n",
        "\n",
        "all_idx = list(range(len(x)))\n",
        "train_idx = set(random.sample(all_idx, int(0.8 * len(all_idx))))\n",
        "test_idx = {idx for idx in all_idx if idx not in train_idx}\n",
        "\n",
        "train_x = x[list(train_idx)].values\n",
        "test_x = x[list(test_idx)].values\n",
        "train_y = y[list(train_idx)].values\n",
        "test_y = y[list(test_idx)].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wQsJ7P2QOp2K",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# TODO: pad all the train/test sequences to a fixed length.\n",
        "# Hint: use pre padding for x and post padding for y (why?).\n",
        "# Hint: for x the maxlen should be MAX_MESSAGE_LEN, but for y, it shall be MAX_MESSAGE_LEN+1 because of the special tokens\n",
        "\n",
        "train_data = pad_sequences(train_x, maxlen=MAX_MESSAGE_LEN, dtype='int32', padding='pre', truncating='pre', value=0.0)\n",
        "train_label = pad_sequences(train_y, maxlen=MAX_MESSAGE_LEN+1, dtype='int32', padding='post', truncating='post', value=0.0)\n",
        "test_data = pad_sequences(test_x, maxlen=MAX_MESSAGE_LEN, dtype='int32', padding='pre', truncating='pre', value=0.0)\n",
        "test_label = pad_sequences(test_y, maxlen=MAX_MESSAGE_LEN+1, dtype='int32', padding='post', truncating='post', value=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9eIxvdsaOp2L",
        "outputId": "af476569-d5f6-4de2-849b-28c6735c9a50",
        "colab": {}
      },
      "source": [
        "print(train_data.shape)\n",
        "print(train_label.shape)\n",
        "print(test_data.shape)\n",
        "print(test_label.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(922121, 50)\n",
            "(922121, 51)\n",
            "(230531, 50)\n",
            "(230531, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9764b6aa-13e3-409e-939e-64f7f7083350",
        "_uuid": "89c827fa1a1aabc990e6b04b4a4839bd04b9b42c",
        "colab_type": "text",
        "id": "82FK-B24GExR"
      },
      "source": [
        "## Model Creation\n",
        "We'll create and compile the model here.  It will consist of the following components:\n",
        "\n",
        "- Shared word embeddings\n",
        "  - A shared embedding layer that turns word indexes (a sparse representation) into a dense/compressed representation.  This embeds both the request from the customer, and also the last words uttered by the model that are fed back into the model.\n",
        "- Encoder RNN\n",
        "  - In this case, a single LSTM layer.  This encodes the whole input sentence into a context vector (or thought vector) that represents completely what the customer is saying, and produces a single output.\n",
        "- Decoder RNN\n",
        "  - This RNN (also an LSTM in this case) decodes the context vector into a string of tokens/utterances.  For each time step, it takes the context vector and the embedded last utterance and produces the next utterance, which is fed back into the model.  More complex and effective models copy the encoder state into the decoder, add more layers of LSTMs, and apply attention mechanisms - but these are out of the scope of this simple example.\n",
        "- Next Word Dense+Softmax\n",
        "  - These two layers take the decoder output and turn it into the next word to be uttered.  The dense layer allows the decoder to not map directly to words uttered, and the softmax turns the dense layer output into a probability distribution, from which we pick the most likely next word.\n",
        "\n",
        "![seq2seq model structure](https://i.imgur.com/JmuryKu.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ee656365-3b04-4dfc-a4dc-a4081ceea82d",
        "_uuid": "46bea6af349b1b67b870eb5977c2bd0c798e2b86",
        "colab_type": "code",
        "id": "TciZpD_jGExT",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Dropout, Embedding, RepeatVector, concatenate, \\\n",
        "    TimeDistributed\n",
        "from tensorflow.python.keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d0a87457-573a-43b8-9a0a-d869a75b86ba",
        "_uuid": "420e809e7f3c9986bf4d3e3804198b8205e4a772",
        "colab_type": "code",
        "id": "xQLVRWzeGExX",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    # TODO: create a shared embedding with the correct input, output dimension and input length.\n",
        "    shared_embedding = Embedding(output_dim=EMBEDDING_SIZE,\n",
        "                                 input_dim=VOCAB_SIZE,\n",
        "                                 input_length=MAX_MESSAGE_LEN,\n",
        "                                 name='embedding',)\n",
        "\n",
        "    # ENCODER\n",
        "    \n",
        "    # TODO: create the encoder Input\n",
        "    encoder_input = Input(shape=(MAX_MESSAGE_LEN,),\n",
        "                          dtype='int32',\n",
        "                          name='encoder_input',)\n",
        "        \n",
        "    # TODO: the embedded input\n",
        "    embedded_input = shared_embedding(encoder_input)\n",
        "    \n",
        "    # TODO: create the LSTM encoder \n",
        "    # Hint: no return_sequences - since the encoder here only produces a single value for the\n",
        "    # input sequence provided.\n",
        "    encoder_rnn = LSTM(\n",
        "        CONTEXT_SIZE,\n",
        "        name='encoder',\n",
        "        dropout=DROPOUT\n",
        "    )\n",
        "    \n",
        "    # repeat the context vector.\n",
        "    context = RepeatVector(MAX_MESSAGE_LEN)(encoder_rnn(embedded_input))\n",
        "    \n",
        "    # DECODER\n",
        "    \n",
        "    # TODO: create the previous word input.\n",
        "    last_word_input = Input(shape=(MAX_MESSAGE_LEN, ),\n",
        "                            dtype='int32',\n",
        "                            name='last_word_input',)\n",
        "    \n",
        "    # TODO: the embedded last word. Note that we use teacher forcing here. \n",
        "    # a.k. we use the oracle previous word instead of the generated previous word during training.\n",
        "    embedded_last_word = shared_embedding(last_word_input)\n",
        "\n",
        "    # TODO: Concatenate the context produced by the encoder and the last word uttered as inputs\n",
        "    # to the decoder.\n",
        "    decoder_input = concatenate([embedded_last_word, context], axis=2)\n",
        "    \n",
        "    # TODO: decode the sequence with LSTM\n",
        "    # return_sequences causes LSTM to produce one output per timestep instead of one at the\n",
        "    # end of the intput, which is important for sequence producing models.\n",
        "    decoder_rnn = LSTM(CONTEXT_SIZE,\n",
        "                       name='decoder',\n",
        "                       return_sequences=True,\n",
        "                       dropout=DROPOUT)\n",
        "    \n",
        "    decoder_output = decoder_rnn(decoder_input)\n",
        "    \n",
        "    # Apply a dense layer\n",
        "    # Hint: TimeDistributed allows the dense layer to be applied to each decoder output per timestep\n",
        "    next_word_dense = TimeDistributed(Dense(int(VOCAB_SIZE / 2), activation='relu'),\n",
        "                                      name='next_word_dense',)(decoder_output)\n",
        "    \n",
        "    # Predict the probability of the next word over the whole vocab. \n",
        "    # Hint: TimeDistributed allows the softmax layer to be applied to each decoder output per timestep\n",
        "    next_word = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'),\n",
        "                                name='next_word_softmax')(next_word_dense)\n",
        "    \n",
        "    return Model(inputs=[encoder_input, last_word_input], outputs=[next_word])\n",
        "\n",
        "s2s_model = create_model()\n",
        "optimizer = Adam(lr=LEARNING_RATE, clipvalue=5.0)\n",
        "s2s_model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EHlklL81Op2S",
        "outputId": "af93cb33-17fa-44d0-e8f7-72af9efbe329",
        "colab": {}
      },
      "source": [
        "s2s_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "last_word_input (InputLayer)    [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_input (InputLayer)      [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 50, 128)      524288      encoder_input[0][0]              \n",
            "                                                                 last_word_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (LSTM)                  (None, 256)          394240      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (None, 50, 256)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 50, 384)      0           embedding[1][0]                  \n",
            "                                                                 repeat_vector[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder (LSTM)                  (None, 50, 256)      656384      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "next_word_dense (TimeDistribute (None, 50, 2048)     526336      decoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "next_word_softmax (TimeDistribu (None, 50, 4096)     8392704     next_word_dense[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 10,493,952\n",
            "Trainable params: 10,493,952\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "37dc7acc-ed74-4933-9182-b4329258d31f",
        "_uuid": "261fdad98295fa395592a5a6bf60806151a51c74",
        "colab_type": "text",
        "id": "pXIrxbuRGExc"
      },
      "source": [
        "## Model Training\n",
        "We'll train the model here.  After each sub-batch of the dataset, we'll test with static input strings to see how the model is progressing in human readable terms.  Its important to have these tests along with traditional model evaluation to provide a better understanding of how well the model is training.\n",
        "\n",
        "It's important to pull test strings from the real distribution of the data, also.  It can be hard to really put yourself in customers' shoes when writing test messages, and you will get non-representative results when you provide test examples that don't fit the true distribution of the input data (when your input text doesn't sound like real customer requests)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "LpKVaIV2ZozL",
        "colab_type": "code",
        "outputId": "5c668f33-6f22-4492-d453-cab58f553a38",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 2036021678029933256\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 4832296960\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 7575339198323864770\n",
            "physical_device_desc: \"device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emPR3M-YZozM",
        "colab_type": "code",
        "outputId": "fc7240db-a39b-4147-e5c5-2ef605a1dfaf",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Nov 09 15:34:19 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 436.48       Driver Version: 436.48       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  GeForce RTX 2060   WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   56C    P2    42W /  N/A |   4948MiB /  6144MiB |     16%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|    0     34936      C   ...ta\\Local\\Continuum\\anaconda3\\python.exe N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDm_9RGHZozO",
        "colab_type": "code",
        "outputId": "235f9688-bd07-4293-b6f8-dd866a50c221",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Currently, memory growth needs to be the same across GPUs\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "              tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "              print(e)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Physical devices cannot be modified after being initialized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "db2eee93-8070-4572-962e-c7225b20d2e4",
        "_uuid": "464355ec34af2c192723e66222c036826ac323f7",
        "colab_type": "code",
        "id": "_gKkezs4GExc",
        "colab": {}
      },
      "source": [
        "def add_start_token(y_array):\n",
        "    \"\"\" Adds the start token to vectors.  Used for training data. \"\"\"\n",
        "    return np.hstack([\n",
        "        START * np.ones((len(y_array), 1)),\n",
        "        y_array[:, :-1],\n",
        "    ])\n",
        "\n",
        "def binarize_labels(labels):\n",
        "    \"\"\" Helper function that turns integer word indexes into sparse binary matrices for \n",
        "        the expected model output.\n",
        "    \"\"\"\n",
        "    return np.array([np_utils.to_categorical(row, num_classes=VOCAB_SIZE)\n",
        "                     for row in labels])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qZOa6rEFOp3U",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "def train_genenerator(epochs=20):\n",
        "    for _ in range(epochs):\n",
        "        batch_ids = [i for i in range(math.floor(train_data.shape[0]/bs))]\n",
        "        random.shuffle(batch_ids)\n",
        "        for i in batch_ids:\n",
        "            temp_label = train_label[bs*i:bs*i+bs]\n",
        "            temp_binary_label = binarize_labels(temp_label[:,1:])\n",
        "            yield [train_data[bs*i:bs*i+bs,:], temp_label[:,:-1]], temp_binary_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "20JXLHmZOp3W",
        "scrolled": false,
        "outputId": "a34492c1-5053-42fe-c7fc-ab3e32e6c658",
        "colab": {}
      },
      "source": [
        "history = s2s_model.fit_generator(train_genenerator(),\n",
        "                    steps_per_epoch=math.floor(train_data.shape[0]/bs),\n",
        "                    epochs=1,\n",
        "                    validation_data=(test_data, test_label),\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "Fail to find the dnn implementation. [Op:CudnnRNN]",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-22-9a1347f913b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                     verbose=1)\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[0;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[0;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m    975\u001b[0m                  outputs['metrics'])\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m               training=training))\n\u001b[0m\u001b[0;32m    253\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m   \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m           \u001b[1;31m# Compute outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m           \u001b[0moutput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m           \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    959\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcan_use_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m           last_output, outputs, new_h, new_c, runtime = cudnn_lstm(\n\u001b[1;32m--> 961\u001b[1;33m               **cudnn_lstm_kwargs)\n\u001b[0m\u001b[0;32m    962\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcudnn_lstm\u001b[1;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     outputs, h, c, _ = gen_cudnn_rnn_ops.cudnn_rnn(\n\u001b[0;32m   1173\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_h\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_c\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m         rnn_mode='lstm')\n\u001b[0m\u001b[0;32m   1175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m   \u001b[0mlast_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnn\u001b[1;34m(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0minput_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             ctx=_ctx)\n\u001b[0m\u001b[0;32m    110\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnn_eager_fallback\u001b[1;34m(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)\u001b[0m\n\u001b[0;32m    196\u001b[0m   \"is_training\", is_training)\n\u001b[0;32m    197\u001b[0m   _result = _execute.execute(b\"CudnnRNN\", 4, inputs=_inputs_flat,\n\u001b[1;32m--> 198\u001b[1;33m                              attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[0;32m    199\u001b[0m   _execute.record_gradient(\n\u001b[0;32m    200\u001b[0m       \"CudnnRNN\", _inputs_flat, _attrs, _result, name)\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
            "\u001b[1;31mUnknownError\u001b[0m: Fail to find the dnn implementation. [Op:CudnnRNN]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UkJ3q8vQOp3d",
        "colab": {}
      },
      "source": [
        "s2s_model.save_weights('models/lstm.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Du4AwyQ2Op3f"
      },
      "source": [
        "### Test out the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-vhIW-qmOp3l",
        "colab": {}
      },
      "source": [
        "model = create_model()\n",
        "model.load_weights('models/lstm.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ce33ef3c-90df-4d3f-aad7-82db7b8cbf38",
        "_uuid": "982ca948fcf372ba6d29cc1872b9667c8e7c678f",
        "colab_type": "code",
        "id": "AcvFSoepGExd",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def respond_to(model, text):\n",
        "    \"\"\" Helper function that takes a text input and provides a text output. \"\"\"\n",
        "    text = text.lower()\n",
        "    input_y = add_start_token(PAD * np.ones((1, MAX_MESSAGE_LEN)))\n",
        "    idxs = pad_sequences([sp.EncodeAsIds(str(text))],\n",
        "                            value=PAD,\n",
        "                            padding='pre',\n",
        "                            truncating='post',\n",
        "                            maxlen=MAX_MESSAGE_LEN)\n",
        "    for position in range(MAX_MESSAGE_LEN - 1):\n",
        "        prediction = model.predict([idxs, input_y]).argmax(axis=2)[0]\n",
        "        input_y[:,position + 1] = prediction[position]\n",
        "    final_prediction = model.predict([idxs, input_y]).argmax(axis=2)[0]                         \n",
        "    return sp.decode_ids(final_prediction.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "18a421a1-41d7-4c87-988d-74fa8dc85992",
        "_uuid": "8ebd6c01cd3a7e75810f9fc48b4d2ad913f9f410",
        "colab_type": "code",
        "id": "w0pDK36SGExk",
        "colab": {}
      },
      "source": [
        "respond_to(model, '''@Delta I just saw the pretty flight attendent! She is so beautiful''')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "eef56338-516e-46ed-94d5-6c7d21279d34",
        "_uuid": "e5f0a5acf6611334291cea8c3440c5dd3badf2f0",
        "colab_type": "code",
        "id": "CqtrqVZ-GExo",
        "outputId": "dec218eb-f5f5-4267-d482-3e6dab2e9f05",
        "colab": {}
      },
      "source": [
        "respond_to(model, '''@sprintcare I can't make calls... wtf''')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@__sn__ please send us a direct message, so that we can further assist you. - lp'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1445794c-c26f-4dc9-a38e-90b98ee206cb",
        "_uuid": "f7e4303f8ee7d2def95c95b166b6fac8727217f5",
        "colab_type": "code",
        "id": "nNnH8BD6GExp",
        "outputId": "cec32e84-2809-4503-8d4f-313b077f0567",
        "colab": {}
      },
      "source": [
        "respond_to(s2s_model, '''Hi Amazon, Thanks for the fast delivery! I am your loyal customer''')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@__sn__ you're welcome! ^td\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCMsdldJOp4p",
        "outputId": "e2de4afc-8c18-452b-fad1-029a16fe8618",
        "colab": {}
      },
      "source": [
        "respond_to(s2s_model, '''Hi there, my macbook keeps hanging, what shall I do?''')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@__sn__ we'd like to help. dm us the device you're using and we'll get started. <url>\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5e8uPd9POp4r",
        "outputId": "9a3543f4-1821-47c1-f107-a78e6870a6e1",
        "colab": {}
      },
      "source": [
        "respond_to(s2s_model, '''Stupid, If you don't refund, I will never use your service again. ''')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@__sn__ hi, we're sorry to hear this. can you dm us your email address so we can look into this?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UU14f83COp4u",
        "colab": {}
      },
      "source": [
        "# TODO: Try some interesting queries and see how your chatbot respond.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bEyUt2DXu7lf",
        "colab": {}
      },
      "source": [
        "# Bonus question: try out more sophisticated model instead of the base\n",
        "# encoder-decoder. You can use attention mechanism or even BERT (it'll be very demanding on the hardware)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}